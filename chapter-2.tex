\documentclass[11pt,twocolumn]{report}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[margin=0.75in]{geometry}
\usepackage{bm} % boldface math symbols
\def\realnumbers{\mathbb{R}}
\def\naturalnumbers{\mathbb{N}}
\newcommand\NoIndent[1]{%
  \par\vbox{\parbox[t]{\linewidth}{#1}}%
}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}
\stepcounter{chapter}
\chapter{Linear Algebra}

\textbf{Linear Algebra}
\begin{itemize}
  \item a branch of mathematics that is widely used throughout science and
    engineering
  \item a form of continuous, not discrete math
  \item only topics relevant to deep learning are covered in the succeeding
    review chapters
  \item read \textit{The Matrix Cookbook} (Petersen and Pedersen, 2006) or
    \textit{Linear Algebra} (Shilov, 1977) for a more thorough discussion
\end{itemize}

\section{Scalars, Vectors, Matrices, and Tensors}

\large\textbf{Scalars:}
\begin{itemize}
  \item a single number that is (by the book's notation) written in lowercase
    alpha italics
  \item "Let $ s \in \realnumbers $ be the slope of the line" while defining a
    real-valued scalar
  \item "Let $ n \in \naturalnumbers $ be the number of units" while defining a
    natural number scalar
\end{itemize}

\large\textbf{Vectors:}
\begin{itemize}
  \item an array of numbers arranged in a particular order
  \item think of vectors as things that describe the position points in space,
    with each element giving the coordinate along a particular axis
  \item one-indexed, not zero-indexed (according, again, to the book's notation)
  \item denoted as $\bm{x}$, lowercase boldface
  \item elements of $\bm{x}$ are denoted by writing its name in italic with a
    subscript, as in \textbf{$x_1, x_2$} and so on
  \item to explicitly identify the elements of a vector, write as:
    \begin{gather}
      \bm{x} = 
      \begin{bmatrix}
        x_1\\
        x_2\\
        \vdots\\
        x_n
      \end{bmatrix}
    \end{gather}
  \item we can define a set containing the indices and write the set as a
    subscript of the vector 
  \item to access $x_1, x_3, x_6$; we define set $S = \{1, 3, 6\}$ and write
    $\bm{x}_S$.
  \item we use the - sign to indicate the complement, so $x_{-S}$ is the vector
    containing all of the elements of $\bm{x}$ except for $x_1, x_2, x_3$
\end{itemize}

\large\textbf{Matrices:}
\begin{itemize}
  \item a 2-D array of numbers
  \item each element is identified by two indices in row, column order
  \item if matrix $\bm{A}$ is of size $m \times n$ we say that $\bm{A} \in
    \realnumbers^{m \times n}$
  \item denoted in uppercase boldface alpha chars as in $\bm{A}$
  \item elements are denoted in italic but not boldface as in
    \textit{A}$_{1,1}$ is upper left entry of $\bm{A}$
  \item ":" as an index is a placeholder for "all"
  \item $\bm{A}_{i,:}$ denotes the i-th row of $\bm{A}$
  \item likewise, $\bm{A}_{:,i}$ denotes the i-th colmn of $\bm{A}$
  \item to explicitly identify the elements of a matrix, write as:
    \begin{gather}
      \bm{x} = 
      \begin{bmatrix}
        A_{1,1} & A_{1, 2}\\
        A_{2,1} & A_{2, 2}\\
      \end{bmatrix}
    \end{gather}
  \item additional notation: $f(\bm{A})_{i, j}$ gives element $(i, j)$ of the
    matrix compted by applying function $f$ to $\bm{A}$
\end{itemize}

\large\textbf{Tensors:}
\begin{itemize}
  \item in some cases we will need an array with more than two axes
  \item an array of numbers on a rectangular grid with a variable number of axes
  \item the element of tensor $\bm{A}$ at coordinates $(i, j, k)$ is
    $\bm{A}_{i, j, k}$
\end{itemize}

\section{Basic Matrix Operations}
\subsection{Transpose}
\begin{flushleft}
  The transpose of a matrix is the mirror image across the main diagonal (an
  element is in the main diagonal if the horizontal index is the same with the
  vertical index). We denote the transpose of a matrix $\bm{A}$ as
  $\bm{A}^\intercal$, and it is defined such that:
\end{flushleft}
\begin{equation}
  (\bm{A}^\intercal)_{i, j} = A_{j, i}
\end{equation}

Notes: 
\begin{itemize}
  \item A vector is a matrix of one row, and so we can write a vector inline as
    a row matrix and use the transpose operator to turn it into a standard
    column vector $\bm{x} = [x_1, x_2, x_3]^\intercal$
  \item a scalar is its own transpose: $a = a^\intercal$
\end{itemize}

\subsection{Basic Basics}
Matrix Addition:\\
$\bm{C} = \bm{B} + \bm{A}$ where $C_{i, j} = A_{i, j} + B{i, j}$ as long as
they have the same 'shape'.\\\\
Matrix and Scalar Multiplication/Addition:\\
$\bm{D} = a \cdot \bm{B} + c$ where $D_{i, j} = a \cdot B_{i, j} + c$.\\\\
Non-conventional convenience notation for deep learning:
\begin{flushleft}
  We allow the addition of a matrix and a vector, yielding another matrix 
  $\bm{C} = \bm{A} + \bm{b}$, where $C_{i, j} = A_{i, j} + b_j$. In simpler
  terms, vector $\bm{b}$ is added to each row of the matrix. This implicit
  copying of $\bm{b}$ to many locations is called \textbf{broadcasting}.
\end{flushleft}

\section{Multiplying Matrices and Vectors}
\subsection{Matrix Multiplication}
One of the most important matrix operations is the multiplication of two matrices. The
\textbf{matrix product} of matrices $\bm{A} \text{ and } \bm{B}$ is a third
matrix $\bm{C}$. In order for this product to be defined, $\bm{A}$ must have
the same columns as $\bm{B}$ has rows. If $\bm{A}$ is $m \times n$, and
$\bm{B}$ is $n \times p$, then $\bm{C}$ is of shape $m \times p$.
The product is denoted by:
\begin{equation}
  \bm{C} = \bm{A}\bm{B}.
\end{equation}
And it is defined by:
\begin{equation}
  C_{i, j} = \sum_k A_{i, k}B_{k, j}.
\end{equation}
Note: This is different from the element-wise product or Hadamard product,
denoted as $\bm{A} \odot \bm{B}$ for matrices of the same dimension $m \times
n$ (Def'n: $(\bm{A} \odot \bm{B})_{i, j} = \bm{A}_{i, j}\bm{B}_{i, j}.$
\subsection{Dot Product}
The dot product is the product of two vectors, and is defined as:
\begin{equation}
  \bm{a} \cdot \bm{b} = \sum_{i = 1}^n a_ib_i
\end{equation}
provided they are both of size $n$.\par
Notes:
\begin{itemize}
  \item Informally, multiply each corresponding element and add them altogether
  \item The dot product is a folding function
  \item can also be thought of as the matrix product $\bm{x}^\intercal\bm{y}$
\end{itemize}
\subsubsection{Properties}
\begin{enumerate}
  \item Distributive: 
    \begin{equation}
      \bm{A}(\bm{B} + \bm{C}) = \bm{AB} + \bm{AC}
    \end{equation}
  \item Associative:
    \begin{equation}
      \bm{A}(\bm{BC}) = (\bm{AB})\bm{C}
    \end{equation}
  \item Matrix multiplication is not commutative. (That is, $\bm{AB} = \bm{BA}$
    does not always hold. However, the dot product is commutative:
    \begin{equation}
      \bm{x}^\intercal\bm{y} = \bm{y}^\intercal\bm{x}
    \end{equation}
  \item Transpose of a matrix product:
    \begin{equation}
      (\bm{AB})^\intercal = \bm{B}^\intercal\bm{A}^\intercal
    \end{equation}
  \item Existence of Multiplication Identity:
    \begin{equation}
      \bm{I}_m\bm{A} = \bm{A}\bm{I}_n = \bm{A}
    \end{equation}
    Provided $\bm{A}$ is of size $m \times n$.
\end{enumerate}

\section{System of Linear Equations}
In matrix form, it is as such:
\begin{equation}
  \label{linear_equation}
  \bm{Ax} = \bm{b}
\end{equation}
where $\bm{A} \in \realnumbers^{m \times n}$ is a known matrix, $\bm{b} \in
\realnumbers^m$ is a known vector, and $\bm{x} \in \realnumbers^n$ is a vector
of unknown variables we would like to solve for. Explicitly written:
\begin{align}
  \bm{A}_{1, :}\bm{x} & = b_1\\
  \bm{A}_{2, :}\bm{x} & = b_2\\
  & \hdots\\
  \bm{A}_{m, :}\bm{x} & = b_m
\end{align}

\section{Identity and Inverse Matrices}
An \textbf{identity matrix} of size $n$ (denoted $I_n$)
is the $n \times n$ square matrix with ones on the main diagonal and
zeroes elsewhere:
\begin{equation*}
  I_1 = [1],
  I_2 = \begin{bmatrix}
    1 & 0\\
    0 & 1
  \end{bmatrix},
  I_3 = \begin{bmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & 0 & 1
  \end{bmatrix}
\end{equation*}
The \textbf{matrix inverse} of $\bm{A}$ is denoted as $\bm{A}^{-1}$, and it defined as:
\begin{align}
  \bm{A}^{-1}\bm{A} & = \bm{I}_n\\
  \bm{A}\bm{A}^{-1} & = \bm{I}_n
\end{align}
We can now solve \ref{linear_equation} by the following steps:
\begin{align}
  \bm{Ax} & = \bm{b}\\
  \bm{A}^{-1}\bm{Ax} & = \bm{A}^{-1}\bm{b}\\
  \bm{I}_n\bm{x} & = \bm{A}^{-1}\bm{b}\\
  \bm{x} & = \bm{A}^{-1}\bm{b}.
\end{align}
Of course, this process hinges on the fact that $\bm{A}^{-1}$ exists.
But in practice, we almost never use this due to the fact that digital
computers can only represent $\bm{A}^{-1}$ with limited precision. Algorithms
that make use of $\bm{b}$ can usually obtain more accurate estimaes of
$\bm{x}$.

\section{Linear Dependence and Span}

In order for the inverse ($\bm{A}^{-1}$) of a matrix $\bm{A}$ to exist, equation
\ref{linear_equation} must have exactly one solution for every value of
$\bm{b}$. But it is also possible for the system to have a) no solutions (no
intersection) or b) infinitely many solutions (colinear). Strictly only cases a
and b are possible for every system of equations.

A good inuition to a solution of a system of linear equations is this: think of
the columns of $\bm{A}$ as specifying different directions we can travel from
the \textbf{origin}, and determine how many ways there are to reach $\bm{b}$.
In this view, each element of $\bm{x}$ specifies how far we should travel in
each of those directions, with $x_i$ specifying how far to move in the
direction of column $i$ in $\bm{A}$:
\begin{equation}
  \bm{Ax} = \sum_i x_i\bm{A}_{:, i}
\end{equation}

In general, this operation is called a \textbf{linear combination}. A linear
combination is simply some set of vectors \{$\bm{v}_1, \bm{v}_2, ..., \bm{v}_n$\}
each multiplied by its own scaling factor c and adding the results:
\begin{equation}
  \sum_i c_i\bm{v}_i
\end{equation}

A \textbf{span} of a set of vectors is the set of all vectors that can be
formed by linear combinations of the original vectors. 
Note: Any two noncollinear vectors can represent the whole $\realnumbers^2$
space.

Thus, $\bm{Ax} = \bm{b}$ amounts to testing whether $\bm{b}$ is in the span of
the columns of $\bm{A}$. This particular span is called the \textbf{column
  space} or the \textbf{range} of $\bm{A}$.

A set of vectors is \textbf{linearly independent} if no vector in the set is a
linear combination of the other vectors.

Notes:
\begin{itemize}
  \item Adding a new vector in a set of vectors that is a linear combination of
    the other vectors in the set does not add any new points to the set's span
  \item For the column space/range of a matrix to encompass all of
    $\realnumbers^m$, the matrix must contain at least one set of $m$ linearly
    independent columns
  \item No set of $m$-dimensional vectors can have more than $m$ mutually
    linearly independent columns
  \item But, a matrix with more than $m$ columns may contain more than one such
    set
  \item In order for a matrix to have an inverse, we need to ensure that
    equation \ref{linear_equation} has \textit{at most} one solution for each
    value of $\bm{b}$, i.e., ensuring that the matrix has at most $m$ columns
\end{itemize}

Tying it all together, this means that the matrix must be \textbf{square}, that
is, $m=n$, and all of the columns must be linearly independent.

Random:
\begin{itemize}
  \item A square matrix with linearly independent columns is called a
    \textbf{singular} matrix 
  \item If $\bm{A}$ is not square or square but not singular, it can stil be
    possible to solve it.  However, matrix inversion cannot be used to find the
    solution
  \item For square matrices the left and right inverse are equal
\end{itemize}

\section{Norms}
\begin{itemize}
  \item the size of a vector is given by a function called the $L^p$ norm:
    \begin{equation}
      \norm{\bm{x}}_p = (\sum_i {\lvert x_i \rvert}^p)^{\frac{1}{p}}
    \end{equation}
  \item norms satisfy the triangle inequality: $f(\bm{x} + \bm{y}) \leq
    f(\bm{x}) + f(\bm{y})$
  \item also, $\forall \alpha \in \realnumbers, f(\alpha\bm{x}) =
    |\alpha|f(\bm{x})$
  \item the $L^2$ norm is called the \textbf{Euclidean norm}
  \item the squared $L^2$ norm, calclated simply as $\bm{x}^\intercal\bm{x}$, is
    more convenient to work with computationally than the $L^2$ norm itself
  \item but in many contexts the squared $L^2$ norm may be undesirable because
    it increases very slowly near the origin
  \item $L^\infty$ norm, the \textbf{max norm}, is the element with the largest
    magnitude in the vector:
    \begin{equation}
      \norm{\bm{x}}_\infty = \max_i |x_i|
    \end{equation}
  \item the size of a matrix is given by the \textbf{Frobenius norm}:
    \begin{equation}
      \norm{\bm{A}}_F = \sqrt{\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^2}
    \end{equation}
    analogous to the Eucledean norm of a vector
\end{itemize}

\section{Special Kinds of Matrices and Vectors}
\subsection{Diagonal Matrices}
\begin{itemize}
  \item Diagonal matrices have non-zero entries only along the main
    diagonal
  \item Formally, matrix $\bm{D}$ is diagonal iff $D_{i, j} = 0 \text{  }
    \forall i \neq j$
  \item we write $diag(\bm{v})$ to denote a square diagonal matrix whose
    entries are given by vector $\bm{v}$
  \item very computationally efficient multiplication and inversion (the
    inverse exists iff every diagonal entry is nonzero)
  \item $diag(\bm{v})\bm{x} = \bm{v} \odot \bm{x}$
  \item $diag(\bm{v})^{-1} =
    diag([\frac{1}{v_1},\frac{1}{v_2},...,\frac{1}{v_n}]^\intercal)$

\end{itemize}

\end{document}
