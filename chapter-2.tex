\documentclass[11pt,twocolumn]{report}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[margin=0.75in]{geometry}
\usepackage{bm} % boldface math symbols
\def\realnumbers{\mathbb{R}}
\def\naturalnumbers{\mathbb{N}}
\def\complexnumbers{\mathbb{C}}
\newcommand\NoIndent[1]{%
  \par\vbox{\parbox[t]{\linewidth}{#1}}%
}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}
\stepcounter{chapter}
\chapter{Linear Algebra}

\textbf{Linear Algebra}
\begin{itemize}
  \item a branch of mathematics that is widely used throughout science and
    engineering
  \item a form of continuous, not discrete math
  \item only topics relevant to deep learning are covered in the succeeding
    review chapters
  \item read \textit{The Matrix Cookbook} (Petersen and Pedersen, 2006) or
    \textit{Linear Algebra} (Shilov, 1977) for a more thorough discussion
\end{itemize}

\section{Scalars, Vectors, Matrices, and Tensors}

\large\textbf{Scalars:}
\begin{itemize}
  \item a single number that is (by the book's notation) written in lowercase
    alpha italics
  \item "Let $ s \in \realnumbers $ be the slope of the line" while defining a
    real-valued scalar
  \item "Let $ n \in \naturalnumbers $ be the number of units" while defining a
    natural number scalar
\end{itemize}

\large\textbf{Vectors:}
\begin{itemize}
  \item an array of numbers arranged in a particular order
  \item think of vectors as things that describe the position points in space,
    with each element giving the coordinate along a particular axis
  \item one-indexed, not zero-indexed (according, again, to the book's notation)
  \item denoted as $\bm{x}$, lowercase boldface
  \item elements of $\bm{x}$ are denoted by writing its name in italic with a
    subscript, as in \textbf{$x_1, x_2$} and so on
  \item to explicitly identify the elements of a vector, write as:
    \begin{gather}
      \bm{x} = 
      \begin{bmatrix}
        x_1\\
        x_2\\
        \vdots\\
        x_n
      \end{bmatrix}
    \end{gather}
  \item we can define a set containing the indices and write the set as a
    subscript of the vector 
  \item to access $x_1, x_3, x_6$; we define set $S = \{1, 3, 6\}$ and write
    $\bm{x}_S$.
  \item we use the - sign to indicate the complement, so $x_{-S}$ is the vector
    containing all of the elements of $\bm{x}$ except for $x_1, x_2, x_3$
\end{itemize}

\large\textbf{Matrices:}
\begin{itemize}
  \item a 2-D array of numbers
  \item each element is identified by two indices in row, column order
  \item if matrix $\bm{A}$ is of size $m \times n$ we say that $\bm{A} \in
    \realnumbers^{m \times n}$
  \item denoted in uppercase boldface alpha chars as in $\bm{A}$
  \item elements are denoted in italic but not boldface as in
    \textit{A}$_{1,1}$ is upper left entry of $\bm{A}$
  \item ":" as an index is a placeholder for "all"
  \item $\bm{A}_{i,:}$ denotes the i-th row of $\bm{A}$
  \item likewise, $\bm{A}_{:,i}$ denotes the i-th colmn of $\bm{A}$
  \item to explicitly identify the elements of a matrix, write as:
    \begin{gather}
      \bm{x} = 
      \begin{bmatrix}
        A_{1,1} & A_{1, 2}\\
        A_{2,1} & A_{2, 2}\\
      \end{bmatrix}
    \end{gather}
  \item additional notation: $f(\bm{A})_{i, j}$ gives element $(i, j)$ of the
    matrix compted by applying function $f$ to $\bm{A}$
\end{itemize}

\large\textbf{Tensors:}
\begin{itemize}
  \item in some cases we will need an array with more than two axes
  \item an array of numbers on a rectangular grid with a variable number of axes
  \item the element of tensor $\bm{A}$ at coordinates $(i, j, k)$ is
    $\bm{A}_{i, j, k}$
\end{itemize}

\section{Basic Matrix Operations}
\subsection{Transpose}
\begin{flushleft}
  The transpose of a matrix is the mirror image across the main diagonal (an
  element is in the main diagonal if the horizontal index is the same with the
  vertical index). We denote the transpose of a matrix $\bm{A}$ as
  $\bm{A}^\intercal$, and it is defined such that:
\end{flushleft}
\begin{equation}
  (\bm{A}^\intercal)_{i, j} = A_{j, i}
\end{equation}

Notes: 
\begin{itemize}
  \item A vector is a matrix of one row, and so we can write a vector inline as
    a row matrix and use the transpose operator to turn it into a standard
    column vector $\bm{x} = [x_1, x_2, x_3]^\intercal$
  \item a scalar is its own transpose: $a = a^\intercal$
\end{itemize}

\subsection{Basic Basics}
Matrix Addition:\\
$\bm{C} = \bm{B} + \bm{A}$ where $C_{i, j} = A_{i, j} + B{i, j}$ as long as
they have the same 'shape'.\\\\
Matrix and Scalar Multiplication/Addition:\\
$\bm{D} = a \cdot \bm{B} + c$ where $D_{i, j} = a \cdot B_{i, j} + c$.\\\\
Non-conventional convenience notation for deep learning:
\begin{flushleft}
  We allow the addition of a matrix and a vector, yielding another matrix 
  $\bm{C} = \bm{A} + \bm{b}$, where $C_{i, j} = A_{i, j} + b_j$. In simpler
  terms, vector $\bm{b}$ is added to each row of the matrix. This implicit
  copying of $\bm{b}$ to many locations is called \textbf{broadcasting}.
\end{flushleft}

\section{Multiplying Matrices and Vectors}
\subsection{Matrix Multiplication}
One of the most important matrix operations is the multiplication of two matrices. The
\textbf{matrix product} of matrices $\bm{A} \text{ and } \bm{B}$ is a third
matrix $\bm{C}$. In order for this product to be defined, $\bm{A}$ must have
the same columns as $\bm{B}$ has rows. If $\bm{A}$ is $m \times n$, and
$\bm{B}$ is $n \times p$, then $\bm{C}$ is of shape $m \times p$.
The product is denoted by:
\begin{equation}
  \bm{C} = \bm{A}\bm{B}.
\end{equation}
And it is defined by:
\begin{equation}
  C_{i, j} = \sum_k A_{i, k}B_{k, j}.
\end{equation}
Notes:
\begin{itemize}
  \item This is different from the element-wise product or Hadamard product,
    denoted as $\bm{A} \odot \bm{B}$ for matrices of the same dimension $m \times
    n$ (Def'n: $(\bm{A} \odot \bm{B})_{i, j} = \bm{A}_{i, j}\bm{B}_{i, j}.$
  \item multiply each element in the rows by each in the columns then add them
    altogether to get a singular entry in the product matrix
\end{itemize}
\subsection{Dot Product}
The dot product is the product of two vectors, and is defined as:
\begin{equation}
  \bm{a} \cdot \bm{b} = \sum_{i = 1}^n a_ib_i
\end{equation}
provided they are both of size $n$.\par
Notes:
\begin{itemize}
  \item informally, multiply each corresponding element and add them altogether
  \item the dot product is a folding function
  \item can also be thought of as the matrix product $\bm{x}^\intercal\bm{y}$
\end{itemize}
\subsubsection{Properties}
\begin{enumerate}
  \item Distributive: 
    \begin{equation}
      \bm{A}(\bm{B} + \bm{C}) = \bm{AB} + \bm{AC}
    \end{equation}
  \item Associative:
    \begin{equation}
      \bm{A}(\bm{BC}) = (\bm{AB})\bm{C}
    \end{equation}
  \item Matrix multiplication is not commutative. (That is, $\bm{AB} = \bm{BA}$
    does not always hold. However, the dot product is commutative:
    \begin{equation}
      \bm{x}^\intercal\bm{y} = \bm{y}^\intercal\bm{x}
    \end{equation}
  \item Transpose of a matrix product:
    \begin{equation}
      (\bm{AB})^\intercal = \bm{B}^\intercal\bm{A}^\intercal
    \end{equation}
  \item Existence of Multiplication Identity:
    \begin{equation}
      \bm{I}_m\bm{A} = \bm{A}\bm{I}_n = \bm{A}
    \end{equation}
    Provided $\bm{A}$ is of size $m \times n$.
\end{enumerate}

\section{System of Linear Equations}
In matrix form, it is as such:
\begin{equation}
  \label{linear_equation}
  \bm{Ax} = \bm{b}
\end{equation}
where $\bm{A} \in \realnumbers^{m \times n}$ is a known matrix, $\bm{b} \in
\realnumbers^m$ is a known vector, and $\bm{x} \in \realnumbers^n$ is a vector
of unknown variables we would like to solve for. Explicitly written:
\begin{align}
  \bm{A}_{1, :}\bm{x} & = b_1\\
  \bm{A}_{2, :}\bm{x} & = b_2\\
  & \hdots\\
  \bm{A}_{m, :}\bm{x} & = b_m
\end{align}

\section{Identity and Inverse Matrices}
An \textbf{identity matrix} of size $n$ (denoted $I_n$)
is the $n \times n$ square matrix with ones on the main diagonal and
zeroes elsewhere:
\begin{equation*}
  I_1 = [1],
  I_2 = \begin{bmatrix}
    1 & 0\\
    0 & 1
  \end{bmatrix},
  I_3 = \begin{bmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & 0 & 1
  \end{bmatrix}
\end{equation*}
The \textbf{matrix inverse} of $\bm{A}$ is denoted as $\bm{A}^{-1}$, and it defined as:
\begin{align}
  \bm{A}^{-1}\bm{A} & = \bm{I}_n\\
  \bm{A}\bm{A}^{-1} & = \bm{I}_n
\end{align}
We can now solve \ref{linear_equation} by the following steps:
\begin{align}
  \bm{Ax} & = \bm{b}\\
  \bm{A}^{-1}\bm{Ax} & = \bm{A}^{-1}\bm{b}\\
  \bm{I}_n\bm{x} & = \bm{A}^{-1}\bm{b}\\
  \bm{x} & = \bm{A}^{-1}\bm{b}.
\end{align}
Of course, this process hinges on the fact that $\bm{A}^{-1}$ exists.
But in practice, we almost never use this due to the fact that digital
computers can only represent $\bm{A}^{-1}$ with limited precision. Algorithms
that make use of $\bm{b}$ can usually obtain more accurate estimaes of
$\bm{x}$.

\section{Linear Dependence and Span}

In order for the inverse ($\bm{A}^{-1}$) of a matrix $\bm{A}$ to exist, equation
\ref{linear_equation} must have exactly one solution for every value of
$\bm{b}$. But it is also possible for the system to have a) no solutions (no
intersection) or b) infinitely many solutions (colinear). Strictly only cases a
and b are possible for every system of equations.

A good inuition to a solution of a system of linear equations is this: think of
the columns of $\bm{A}$ as specifying different directions we can travel from
the \textbf{origin}, and determine how many ways there are to reach $\bm{b}$.
In this view, each element of $\bm{x}$ specifies how far we should travel in
each of those directions, with $x_i$ specifying how far to move in the
direction of column $i$ in $\bm{A}$:
\begin{equation}
  \bm{Ax} = \sum_i x_i\bm{A}_{:, i}
\end{equation}

In general, this operation is called a \textbf{linear combination}. A linear
combination is simply some set of vectors \{$\bm{v}_1, \bm{v}_2, ..., \bm{v}_n$\}
each multiplied by its own scaling factor c and adding the results:
\begin{equation}
  \sum_i c_i\bm{v}_i
\end{equation}

A \textbf{span} of a set of vectors is the set of all vectors that can be
formed by linear combinations of the original vectors. 
Note: Any two noncollinear vectors can represent the whole $\realnumbers^2$
space.

Thus, $\bm{Ax} = \bm{b}$ amounts to testing whether $\bm{b}$ is in the span of
the columns of $\bm{A}$. This particular span is called the \textbf{column
  space} or the \textbf{range} of $\bm{A}$.

A set of vectors is \textbf{linearly independent} if no vector in the set is a
linear combination of the other vectors.

Notes:
\begin{itemize}
  \item Adding a new vector in a set of vectors that is a linear combination of
    the other vectors in the set does not add any new points to the set's span
  \item For the column space/range of a matrix to encompass all of
    $\realnumbers^m$, the matrix must contain at least one set of $m$ linearly
    independent columns
  \item No set of $m$-dimensional vectors can have more than $m$ mutually
    linearly independent columns
  \item But, a matrix with more than $m$ columns may contain more than one such
    set
  \item In order for a matrix to have an inverse, we need to ensure that
    equation \ref{linear_equation} has \textit{at most} one solution for each
    value of $\bm{b}$, i.e., ensuring that the matrix has at most $m$ columns
\end{itemize}

Tying it all together, this means that the matrix must be \textbf{square}, that
is, $m=n$, and all of the columns must be linearly independent.

Random:
\begin{itemize}
  \item a square matrix with linearly independent columns is called a
    \textbf{singular} matrix 
  \item if $\bm{A}$ is not square or square but not singular, it can stil be
    possible to solve it.  However, matrix inversion cannot be used to find the
    solution
  \item for square matrices the left and right inverse are equal
\end{itemize}

\section{Norms}
\begin{itemize}
  \item the size of a vector is given by a function called the $L^p$ norm:
    \begin{equation}
      \norm{\bm{x}}_p = (\sum_i {\lvert x_i \rvert}^p)^{\frac{1}{p}}
    \end{equation}
  \item norms satisfy the triangle inequality: $f(\bm{x} + \bm{y}) \leq
    f(\bm{x}) + f(\bm{y})$
  \item also, $\forall \alpha \in \realnumbers, f(\alpha\bm{x}) =
    |\alpha|f(\bm{x})$
  \item the $L^2$ norm is called the \textbf{Euclidean norm}
  \item the squared $L^2$ norm, calclated simply as $\bm{x}^\intercal\bm{x}$, is
    more convenient to work with computationally than the $L^2$ norm itself
  \item but in many contexts the squared $L^2$ norm may be undesirable because
    it increases very slowly near the origin
  \item $L^\infty$ norm, the \textbf{max norm}, is the element with the largest
    magnitude in the vector:
    \begin{equation}
      \norm{\bm{x}}_\infty = \max_i |x_i|
    \end{equation}
  \item the size of a matrix is given by the \textbf{Frobenius norm}:
    \begin{equation}
      \norm{\bm{A}}_F = \sqrt{\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^2}
    \end{equation}
    analogous to the Eucledean norm of a vector
\end{itemize}

\section{Special Kinds of Matrices and Vectors}
\subsection{Diagonal Matrices}
\begin{itemize}
  \item Diagonal matrices have non-zero entries only along the main
    diagonal $m_{ij}$ with $i = j$ (this applies to non-square matrices too)
  \item formally, matrix $\bm{D}$ is diagonal iff $D_{i, j} = 0 \text{  }
    \forall i \neq j$
  \item we write $diag(\bm{v})$ to denote a square diagonal matrix whose
    entries are given by vector $\bm{v}$
  \item very computationally efficient multiplication and inversion (the
    inverse exists iff every diagonal entry is nonzero)
  \item $diag(\bm{v})\bm{x} = \bm{v} \odot \bm{x}$ (scale each element $x_i$ by
    $v_i$
  \item $diag(\bm{v})^{-1} =
    diag([\frac{1}{v_1},\frac{1}{v_2},...,\frac{1}{v_n}]^\intercal)$
    (reciprocal each element and put them all in a new diagonal matrix)
  \item non-square diagonal matrices do not have inverses but it is still useful for
    computationally-efficient multiplication
  \item pre-multiplying a diagonal matrix to a square matrix of the same size
    \textbf{scales} each element of the square matrix row-wise with the
    diagonal matrix:
    \begin{scriptsize}
      \begin{equation*}
        \begin{bmatrix}
          k_1 & 0 & \hdots & 0\\
          0 & k_2 & \hdots & 0\\
          \hdots & \hdots & \hdots & \hdots\\
          0 & 0 & \hdots & k_m
        \end{bmatrix}
        \begin{bmatrix}
          a_{11} & a_{12} & \hdots & a_{1n}\\
          a_{21} & a_{22} & \hdots & a_{2n}\\
          \hdots & \hdots & \hdots & \hdots\\
          a_{m1} & a_{m2} & \hdots & a_{mn}\\
        \end{bmatrix}
        =
        \begin{bmatrix}
          k_1a_{11} & k_1a_{12} & \hdots & k_1a_{1n}\\
          k_2a_{21} & k_2a_{22} & \hdots & k_2a_{2n}\\
          \hdots & \hdots & \hdots & \hdots\\
          k_ma_{m1} & k_ma_{m2} & \hdots & k_ma_{mn}\\
        \end{bmatrix}
      \end{equation*}
    \end{scriptsize}
  \item post-multiplying a diagonal matrix scales each element of the square
    matrix column-wise with the diagonal matrix
\end{itemize}

\subsection{Symmetric Matrices}
\begin{itemize}
  \item a symmetric matrix is any \textbf{square matrix} that is equal to its
    own tranpose:
    \begin{equation}
      \bm{A} = \bm{A}^\intercal
    \end{equation}
  \item often arise when the entries are generated by some function of two
    arguments that does not depend on the order of these arguments
  \item ex: If $\bm{A}$ is a matrix of distance measurements, with $\bm{A}_{i,
      j}$ giving the distance from point $i$ to point $j$, then $\bm{A}_{i,j} =
    \bm{A}_{j, i}$
  \item entries are symmetric with respect to the main diagonal:
    \begin{equation*}
      \begin{bmatrix}
        1 & 7 & 3 \\
        7 & 4 & -5 \\
        3 & -5 & 6
      \end{bmatrix}
    \end{equation*}
  \item all square diagonal matrices are symmetric
\end{itemize}

\subsection{Unit vector}
\begin{itemize}
  \item a \textbf{unit vector} is a vector with a \textbf{unit norm}; that is,
    a norm equal to one:
    \begin{equation}
      \norm{\bm{x}}_2 = 1
    \end{equation}
  \item written with a circumflex, or more colloqually known as the "hat":
    $\hat{\bm{i}}$
  \item the \textbf{normalized vector} $\hat{\bm{u}}$ of a non-zero vector
    $\bm{u}$ is the unit vector in the direction of $\bm{u}$:
    \begin{equation}
      \hat{\bm{u}} = \frac{\bm{u}}{|\bm{u}|}
    \end{equation}
\end{itemize}

\subsection{Orthogonal Matrices and Vectors}
\begin{itemize}
  \item two vectors are orthogonal if $\bm{x}^\intercal\bm{y} = 0$ ($\cos(90) =
    0$)
  \item in $\realnumbers^n$, at most $n$ vectors may be mutually orthogonal
    with nonzero norm
    \\
    \\
    \\
    \\
  \item vectors that are of unit norm and are orthogonal are called
    \textbf{orthonormal}
  \item an orthogonal matrix is a square matrix whose columns and rows are
    orthogonal unit vectors (orthonrmal):
    \begin{equation}
      \bm{A}^\intercal\bm{A} = \bm{A}\bm{A}^\intercal = \bm{I}
    \end{equation}
    which implies:
    \begin{equation}
      \bm{A}^{-1} = \bm{A}^\intercal
    \end{equation}
  \item orthogonal matrices are of interest because their inverse is easy to
    compute
\end{itemize}

\section{Eigendecomposition}
\begin{itemize}
  \item many mathematical objects can be understood better by breaking them
    into constituent parts
  \item we can decompose matrices in ways that show us more information about
    their functional properties that is not immediately obvious from the
    standard representation of a matrix
  \item one of the most widely-used kinds of matrix decomposition is
    \textbf{eigendecomposition}
  \item in the following equation:
    \begin{equation}
      \label{eigendecomposition}
      \bm{Av} = \lambda\bm{v}
    \end{equation}
  \item an \textbf{eigenvector} of a square matrix $\bm{A}$ is a non-zero
    vector $\bm{v}$ such that the multiplication by $\bm{A}$ alters only the
    scale of $\bm{v}$
  \item the scalar $\lambda$ is known as the \textbf{eigenvalue} corresponding
    to this eigenvector
  \item $\lambda \in \realnumbers$, and even $\lambda \in \complexnumbers$!
  \item \textbf{eigenvectors are vectors, which, after being applied with a
      transformation matrix, may scale in length (change magnitude), but do NOT
      change in direction}
  \item \textbf{the amounts in which eigenvectors strech are called eigenvalues!}
\end{itemize}
Properties:
\begin{itemize}
  \item different eigenvectors correspond to different eigenvalues for the same
    $\bm{A}$
  \item if $\bm{v}$ is an eigenvector for $\bm{A}$, then so is any rescaled
    vector $s\bm{v}$ for $s \in \realnumbers, s \neq 0$
  \item moreoever, $s\bm{v}$ still has the same eigenvalue
  \item for this reason we look for unit eigenvectors
  \item \textbf{left eigenvector}: $\bm{v}^\intercal\bm{A} =
    \lambda\bm{v}^\intercal$ (but we are usually concerned with right
    eigenvectors)
\end{itemize}
\textbf{Eigendecomposition:}\\
Suppose that a matrix $\bm{A}$ has $n$ linearly independent eigenvectors,
$\{\bm{v}^{(1)},...,\bm{v}^{(n)}\}$, with corresponding eigenvectors
$\{\lambda_1,...,\lambda_n\}$.

We may concatenate the eigenvectors to form matrix $\bm{V}$ with one
eigenvector per column: 
\begin{equation*}
  \bm{V} = [\bm{v}^{(1)},...,\bm{v}^{(n)}]
\end{equation*}

Likewise, concatenate all of the eigenvalues column-wise:
\begin{equation*}
  \bm{\lambda} = [\lambda_1,...,\lambda_n]^\intercal
\end{equation*}

The \textbf{eigendecomposition} is then given by:
\begin{equation}
  \bm{A} = \bm{V}diag(\bm{\lambda})\bm{V}^{-1}
\end{equation}

Keep in mind that we are \textit{constructing} eigendecomposition with
eigenvectors and eigenvalues we know beforehand.

Every real symmetric matrix can be decomposed into an expression using only
real-valued eigenvectors and eigenvales:
\begin{equation}
  \bm{A} = \bm{Q}\bm{\lambda}\bm{Q}^{-1}
\end{equation}
where $\bm{Q}$ is an orthogonal matrix composed of eigenvectors of $\bm{A}$,
and $\bm{\lambda}$ is a diagonal matrix.

An $n \times n$ \textbf{diagonizable} matrix must have a set of $n$ linearly
independent eigenvectors -- the columns of the diagonalizing matrix are such a
set.

Miscellaneous:
\begin{itemize}
  \item a matrix is singular iff any of the eigenvalues are zero
  \item all positive eigenvalues: \textbf{positive definite}
  \item all positive or zero-valued: \textbf{positive semidefinite}
  \item all negative eigenvalues: \textbf{negative definite}
  \item all negative or zero-valued: \textbf{negative semidefinite}
\end{itemize}

\section{Singular Value Decomposition}
\begin{itemize}
  \item factorizes a matrix into \textbf{singular vectors} and \textbf{singular
      values}
  \item more generally applicable as opposed to eigendecomposition
  \item every real matrix has a singular value decomposition, but the same is
    not true for eigendecomposition
  \item SVD:
    \begin{equation}
      \bm{A} = \bm{UDV}^\intercal
    \end{equation}
    where: $\bm{U}$ is an $m \times m$ orthogonal matrix, $\bm{D}$ to be an $m
    \times n$ square (or non-square) diagonal matrix, and $\bm{V}$ to be an $n
    \times n$ orthogonal matrix.
  \item the elements of $\bm{D}$ are known as the \textbf{singular values} of
    $\bm{A}$
  \item the columns of $\bm{U}$ are known as the \textbf{left-singular vectors}
  \item the columns of $\bm{V}$ are known as the \textbf{right-singular vectors}
  \item the most useful feature of SVD is that we can use it to partially
    generalize matrix inversion to non-square matrices
\end{itemize}

\section{The Moore-Penrose Pseudoinverse}
Matrix inversion is not defined for non-square matrices. Suppose we want to
make a left-inverse $\bm{B}$ of a non-square matrix $\bm{A}$, so that we can
solve a linear equation:
\begin{equation}
  \bm{Ax} = \bm{y}
\end{equation}
by left-multiplying each side to obtain:
\begin{equation}
  \bm{x} = \bm{By}
\end{equation}
The Moore-Pensrose Pseudoinverse allows us to make headway in these cases:
\begin{equation}
  \bm{A}^+ = \bm{VD}^+\bm{U}^\intercal
\end{equation}
where $\bm{U}$, $\bm{D}$, and $\bm{V}$ are the singular value decomposition of
$\bm{A}$, and the pseudoinverse $\bm{D}^+$ of a diagonal matrix $\bm{D}$ by
taking the reciprocal of its non-zero element then taking the tranpose of the
resulting matrix.

\section{The Trace Operator}
The trace is the sum of all the diagonal entries of a matrix:
\begin{equation}
  Tr(\bm{A}) = \sum_i \bm{A}_{i, i}
\end{equation}
The Frobenius norm can be written as:
\begin{equation}
  \norm{\bm{A}}_F = \sqrt{Tr(\bm{AA}^\intercal)}
\end{equation}
Properties:
\begin{itemize}
  \item invariant to the transpose operator: $Tr(\bm{A}) = Tr(\bm{A}^\intercal)$
  \item the traces of the cyclic permutations of the products of $n$ matrices,
    assuming that the products exist are the same
  \item a scalar is its own trace: $a = Tr(a)$
\end{itemize}

\section{The Determinant}
\begin{itemize}
  \item the determinant (only exists for square matrices), denoted as
    det($\bm{A}$), is a function mapping matrices to scalars $s \in
    \realnumbers$
  \item the determinant is equal to the product of all the eigenvalues of the
    matrix
  \item \textbf{the absolute value of the determinant can be thought of as the measure
    of how much the matrix expands or contracts when multiplied with a
    transforamtion matrix}
  \item if det($\bm{A}$) $= 0$ then space is contracted completely along at
    least one dimension, causing it to lose all of its volume
  \item if det($\bm{A}$) $= 0$ then the transformation preserves volume
\end{itemize}
\end{document}
