\documentclass[11pt, twocolumn]{report}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[margin=0.75in]{geometry}
\def\expectation{\mathbb{E}}

\begin{document}
\setcounter{chapter}{2}
\chapter{Probability and Information Theory}
Probability Theory
\begin{itemize}
  \item a mathematical framework for representing uncertain statements
  \item provides a means of quantifing uncertainty
  \item allows us to reason in the presence of uncertainty
  \item this is useful for AI systems because the laws of probability tell us
    how they should reason, also for analyzing their behavior
  \item was originally developed to analyze the frequencies of events
  \item defines a set of formal rules for determining the likelihood of a
    proposition being true given the likelihood of other propositions
\end{itemize}

Information Theory
\begin{itemize}
  \item allows us to quantify the amount of uncertainty in a probability
    distribution
\end{itemize}

\section{Why Probability?}
\begin{itemize}
  \item even though many branches of computer science deal mostly with entities
    that are deterministic/certain, machine learning ALWAYS deals with
    uncertain and sometimes stochastic quantities
  \item nearly all activities require some ability to reason amidst uncertainty
  \item in fact it is hard to think of examples beyond mathematical statements
    (that are true by definition) 
\end{itemize}

\textbf{(3)} possible sources of uncertainty:
\begin{enumerate}
  \item inherent stochasticity in the system being modeled
  \item incomplete observability (even deterministic systems can appear
    stochastic when we cannot observe all the variables the drive the system's
    behavior)
  \item incomplete modeling (when we use a model that must discard some of the
    information we have observed, the discarded information can result in
    uncertainty in the model's predictions)
\end{enumerate}

\textbf{Frequentist probability}:
\begin{itemize}
  \item related directly to the rates at which \textit{repeatable} events occur
  \item events like drawing a certain hand of cards in a poker game (repeatable)
\end{itemize}

\textbf{Bayesian probability}:
\begin{itemize}
  \item related to the qualitative levels of uncertainty (\textbf{degree of
      belief})
  \item like when a doctor says that a patient has a 40\% chance of having the
    flu (degree of belief = 40 percent)
\end{itemize}

It is proven that Bayesian probabilities are controlled by the same axioms as
the ones that govern Frequentist probabilities. (Ramsey, 1926)

\section{Random Variables}
\begin{itemize}
  \item a \textbf{random variable} is a variable that can take on different
    variables randomly
  \item typically denoted by a lowercase letter in plain typeface, and the
    values it can take on with lowercase script letters
  \item random variable x, with possible values $x_1$ and $x_2$
  \item on its own, it is merely a description of which states are possible 
  \item it must be coupled with a probability distribution that specifies how
    likely each of these states are
  \item may be discrete or continuous
  \item a discrete random variable is one that has finite or countably
    infinite states (not necessarily the integers, can also just be named
    states)
  \item a continuous random variable is associated with a real value
\end{itemize}

\section{Probability Distributions}
A \textbf{probability distribution} is a description of how likely a random
variable or a set of random variables is to take on each of its possible
states.

\subsection{Discrete Variables and Probability Mass Functions}
\begin{itemize}
  \item a probability distribution over discrete variables may be described
    using a \textbf{probability mass function} (PMF)
  \item we denote them with a capital $P$
  \item the PMF maps from a state of a random variable to the probability of
    the random variable taking on that state
  \item in other words, the probability that x = $x$ is $P(x)$, with a
    probability of 1 indicating that x = $x$ is certain and a probability of 0
    indicating that x = $x$ is impossible
  \item to disambiguate, sometime we write the name of the random variable
    inside the function's argument explicitly: $P(\text{x} = x)$
  \item states that a random variable x comes from a particular distribution: x
    $ \sim P(x)$
  \item joint PMFs act on many variables simultaneously: $P(\text{x} = x,
    \text{y} = y)$, can also be written as $P(x, y)$ for brevity
\end{itemize}
\textbf{(3)} properties for a function $P$ to be a PMF:
\begin{enumerate}
  \item The domain of $P$ must be the set of all possible values of x.
  \item $\forall x \in \text{x}, 0 \leq P(x) \leq 1$. 
  \item $\sum_{x \in \text{x}} P(x) = 1$. We refer to this property as being
    \textbf{normalized}.
\end{enumerate}

\subsection{Continuous Variables and Probability Density Functions}
A \textbf{probability density function} (PDF) is needed when continuous random
varibles are involved rather than a PMF. 

\textbf{(3)} properties for a function $p$ to be a PDF:
\begin{enumerate}
  \item The domain of $p$ must be the set of all possible states of x.
  \item $\forall x \in \text{x}, p(x) \geq 0$. Note that we do not require
    $p(x) \leq 1$.
  \item $\int_{-\infty}^{\infty} p(x)dx = 1$.
\end{enumerate}

Note that a PDF $p(x)$ does not give the probability of a particular state
directly; instead it gives the probability of landing in an infinitesimal
region with volume $\delta x$ is given by $p(x)\delta x$.

The probability that $x$ lies in the interval $[a, b]$ is given by $\int_{[a,
  b]} p(x)dx$.

\section{Marginal Probability}
\begin{itemize}
  \item used when we know the probability distribution across a set of
    variables and we want to know the probability distribution over just a
    subset of them
  \item the probability distribution over the subset is known as the
    \textbf{marginal probability distribution}
  \item discrete variables (derived by the \textbf{sum rule}:
    \begin{equation}
      \forall x \in \text{x}, P(\text{x} = x) = \sum_y P(\text{x} = x, \text{y}
      = y)
    \end{equation}
  \item continuous variables:
    \begin{equation}
      p(x) = \int p(x, y)dy
    \end{equation}
\end{itemize}

\section{Conditional Probability}
\begin{itemize}
  \item the probability of some event, given that some other event has happened
  \item known as the \textbf{conditional probability}
  \item $P(\text{y} = y \text{ } | \text{ x} = x)$
  \item the conditional probability y = $y$ given x = $x$
  \item and is given by the formula:
    \begin{equation}
      P(\text{y} = y \text{ } | \text{ x} = x) = 
      \frac{P(\text{y} = y, \text{x} = x)}{P(\text{x} = x)}
    \end{equation}
  \item the numerator of the above formula is the \textit{joint probability} of
    A and B
  \item the conditional probability is only defined when $P(\text{x} = x) > 0$
  \item because we cannot compute the conditional probability hinged on an
    event that never occurs
\end{itemize}

\section{The Chain Rule of Conditional Probabilities}
Any joint probability distribution over $n$ random variables may be decomposed
into conditional distributions over only one variable:
\begin{equation}
  P(\text{x}^{(1)},...,\text{x}^{(n)}) = P(\text{x}^{(1)} \prod_{i = 2}^{n}
  P(\text{x}^{(i)} \text{ } | \text{ } \text{x}^{(1)},..., \text{x}^{(i-1)})
\end{equation}
This is called the \textbf{chain rule} or the \textbf{product rule} of
probability.\\
Given this, $P(a, b, c) = P(a | b,c)P(b|c)P(c)$.

\section{Independence and Conditional Independence}
Independence:
\begin{itemize}
  \item two random variables x and y are \textbf{independent} if their
    probability distribution can be expressed as a product of two factors, one
    involving only x and one involving only y:
    \begin{equation}
      \forall x \in \text{x}, y \in \text{y}, p(\text{x} = x, \text{y} = y) =
      p(\text{x} = x)p(\text{y} = y)
    \end{equation}
\end{itemize}
lmao just look up conditional independence it's too wordy to type

\section{Expectation, Variance, and Covariance}
\subsection{Expectation}
\begin{itemize}
  \item the \textbf{expectation}, or \textbf{expected value}, of some function
    $f(x)$ with respect to a probability distribution $P(x)$ is the average, or
    mean value, that $f$ takes on when $x$ is drawn from $P$.
  \item discrete:
    \begin{equation}
      \expectation_{\text{x} \sim P}[f(x)] = \sum_x P(x)f(x)
    \end{equation}
  \item continuous:
    \begin{equation}
      \expectation_{\text{x} \sim P}[f(x)] = \int p(x)f(x)
    \end{equation}
  \item by default, we assume that $\expectation{[cdot]}$ averages over all the
    values of the random variables inside the brackets
  \item Expectations are linear:
    \begin{equation}
      \expectation{[\alpha f(x) + \beta g(x)]} = \alpha \expectation
      {[f(x)]} + \beta \expectation{[g(x)]}
    \end{equation}
    where $\alpha$ and $\beta$ are not dependent on $x$
\end{itemize}

\subsection{Variance}
\begin{itemize}
  \item gives a measure of how much the values of a function of a random
    variable x vary as we sample different values of $x$ from its probability
    distribution:
    \begin{equation}
      Var(f(x)) = \expectation{[(f(x)- \expectation{[f(x)]})^{2}]}
    \end{equation}
  \item when the variance is low, the values of $f(x)$ cluster near their
    expected value
  \item the square root of the variance is called the \textbf{standard
      deviation}
\end{itemize}

\subsection{Covariance}
\begin{itemize}
  \item gives some sense of how much two values are linearly related to each
    other, as well as the scale of these variables:
    \begin{equation}
      Cov(f(x),g(y)) = \expectation{[(f(x) - \expectation{[f(x)]})(g(y) -
        \expectation{[g(y)]})]}
    \end{equation}
  \item high values of the covariance mean that the values change very much and
    are both far from their respective means at the same time
  \item if the sign of the covariance is positive, then both variables tend to
    take on relatiely high values simultaneously
  \item if the sign of the covariance is negative, 2then one variable tends to
    take on a relatively high value at the times that the other takes on a
    relatively low value and vice versa
\end{itemize}

\section{Common Probability Distirbutions}
\subsection{Bernoulli Distribution}
A distribution over a single binary random variable. It is controlled by a
single parameter $\phi \in [0, 1]$, which gives the probability of the random
variable being equal to 1. It has the following properties:
\begin{align}
  P(\text{x} = 1) &= \phi\\
  P(\text{x} = 0) &= 1 - \phi\\
  P(\text{x} = x) &= \phi^x(1 - \phi)^{1 - x}\\
  \expectation{}_x[\text{x}] &= \phi\\
  \text{Var}_x(\text{x}) &= \phi(1 - \phi)
\end{align}
\subsection{Multinoulli Distribution}
A distribution over a single discrete variable with $k$ different states, where
$k$ is finite.
\subsection{Gaussian/Normal Distribution}
\begin{itemize}
  \item the most commonly-used distribution over real numbers is the
    \textbf{normal distribution}:
    \begin{equation}
      P(x) = \frac{1}{{\sigma \sqrt {2\pi } }}e^{{{ - \left( {x - \mu } \right)^2 }
          \mathord{\left/ {\vphantom {{ - \left( {x - \mu } \right)^2 } {2\sigma ^2
                  }}} \right. \kern-\nulldelimiterspace} {2\sigma ^2 }}}
    \end{equation}
  \item normal distributions are a sensible choice for many applications, in the
    absence of prior knowledge about what form a distribution over the real
    numbers should take
  \item the \textbf{central limit theorem} shows that the sum of many
    independent random variables is approximately normally distributed (even if
    the original variables themseleves are not normally distributed)
  \item so many of the distributions are truly close to being normal
    distributions
\end{itemize}
\end{document} 
