\documentclass[11pt, twocolumn]{report}
\usepackage[margin=0.75in]{geometry}
\usepackage{amssymb}
\usepackage[]{algorithm2e}
\usepackage{amsmath}
\usepackage{bm}
\def\realnumbers{\mathbb{R}}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\setcounter{chapter}{3}

\chapter{Numerical Computation}
Machine learning algorithms typically require a high amount of numerical
computation. This typically refers to algorithms that update estimates of the
solution via an iterative process, rather than analytically deriving a formula
toprovide a symbolic expression for the correct (and exact) solution. Common
operations include \textbf{optimization} (finding a value of an argument that
minimizes or maximizes a function) and \textbf{solving systems of linear
  equations}. The mere evaluation of a mathematical function on a computer can
be difficult when the function's domain involves real numbers, which cannot be
represented precisely using a finite amount of memory.

\section{Overflow and Underflow}
The fundamental difficulty in performing continuous math operations on a
digital computer is that we need to represent infinitely many digits of a real
number with a finite number of bit patterns, depending on what is available on
a particular machine. This means that for almost all real numbers, we incur
some approximation error just by representing the number in the computer. In
most cases this is rounding error.  Rounding error can cause algorithms that
work in theory to fail in practice if they are not designed to minimize the
accumulation of rounding error.

\textbf{Underflow}
\begin{itemize}
  \item underflow occurs when numbers near zero are rounded to zero
  \item this is a problem because we want to avoid dividing by zero, among
    other things
\end{itemize}

\textbf{Overflow}
\begin{itemize}
  \item overflow occurs when numbers with a large magnitude are approximated as
    $\infty$ or $-\infty$
  \item further arithmetic will usually change these values to NaN values
\end{itemize}

Developers can simply rely on low-level libraries that provide stable
implementations.  Theano is an example of a software package that automatically
detects and stabilizes many common numerically unstable expressions that arise
in the context of deep learning.

\section{Poor Conditioning}
\begin{itemize}
  \item conditining refers to how rapidly a function changes with respect to
    small changes in its inputs
  \item \textbf{rounding errors in the inputs can cause large changes in the
      output}
  \item Consider the function $f(\bm{x}) = \bm{A}^{-1}\bm{x}$. When $\bm{A} \in
    \realnumbers^{n \times n}$ has an eigenvalue decomposition, its
    \textbf{condition number} is:
    \begin{equation}
      \max_{i, j} \left|\frac{\lambda_i}{\lambda_j}\right|
    \end{equation}
    which is the ratio of the largest and smallest eigenvalue
  \item when the condition number is large, matrix inversion is particularly
    sensitive to error in the input
  \item note that this sensitivity is an intrinsic property of the matrix
    itself, not the result of rounding error during matrix inversion
\end{itemize}

\section{Gradient-Based Optimization}
\begin{itemize}
  \item most deep learning algorithms involve optimization of some sort
  \item optimization refers to the task of either minimizing or maximizing
    some function $f(\bm{x})$ by altering $\bm{x}$
  \item most applications focus on minimizing $f$ though
  \item the function we want to minimize is called the \textbf{objective
      function/criterion/cost function/loss function/error function}
  \item note that some ML publications assign special meaning to each of said
    terms
  \item we typically denote the value that minimizes/maximizes a function with
    a superscript $*$, for instance, $\bm{x}^* = \argmin f(\bm{x})$
  \item the \textbf{derivative} is, in simplest terms, the slope of $f$
  \item the \textbf{derivative} specifies how to scale a small change in the
    input to obtain the corresponding change in the output: $f(x + \epsilon)
    \approx f(x) + \epsilon f'(x)$
  \item when $f'(x) = 0$, we are at a \textbf{critial point}
  \item local minimum: a point where $f(x)$ is lower compared to all
    neighboring points, so it is no longer possible to decrease $f(x)$ by taking
    infinitesimal steps
  \item absolute minimum: the absolute lowest value of $f(x)$
  \item local maximum, absolute maximum
  \item critical points that are neither maxima nor minima are called
    \textbf{saddle points}, which have neighbors that are both higher and lower
    than the point itself
\end{itemize}

\subsection{Gradient Descent}
\begin{itemize}
  \item a first-order iterative optimization algorithm for finding the minimum
    of a function
  \item "\textit{reduce $f(x)$ by moving in small steps with the opposite sign
      of the derivative}"
  \item consicely, $f(x - \epsilon \text{sign}(f'(x)))$
\end{itemize}

\textbf{Why the heck must we know this?}
\begin{itemize}
  \item in the context of deep learning, we optimize functions that may have
    many local minima that aren't optimal and many saddle points surrounded by
    very flat regions
  \item this makes optimization difficult
  \item not to mention when the input is multidimensional
  \item we therefore usually settl for a value of $f$ that is very low but may
    or may not be necessarily a minima in the formal sense
  \item we often minimize functions that have multiple inputs $f :
    \realnumbers^n \to \realnumbers$ (for the minimization to make sense the
    output must be scalar -- singular)
\end{itemize}

\subsection{Gradient Descent - Multiple Inputs}
\begin{itemize}
  \item for functions with multiple inputs we use \textbf{partial derivatives}
  \item the partial derivative $\frac{\partial}{\partial x_i} f(\bm{x})$
    measure how $f$ changes as only the variable $x_i$ inreases at point
    $\bm{x}$
  \item the \textbf{gradient} of $f$ is then the vector containing all the
    partial derivatives, denoted $\nabla_{\bm{x}} f(\bm{x})$
  \item the gradient is then the multi-variable generalization of the derivative
  \item element $i$ of the gradient is the partal derivative of $f$ with
    respect to $x_i$
  \item *in multiple dimensions, critical points are points where every element
    of the gradient is equal to zero
  \item there are many methods in this area, most notably, \textbf{steepest
      descent}, \textbf{line search}
  \item gradient descent is limited to optimization in continuous
    spaces, but the generally concept of repeatedly making a small move toward
    better configs can be generalized to discrete spaces
\end{itemize}

\subsection{Beyond the Gradient: Jacobian and Hessian Matrices}
\subsubsection{Jacobian matrix}
\begin{itemize}
  \item the matrix containing all partial derivatives of a function $f$ is
    called the \textbf{Jacobian matrix}
  \item specifically: if we have a function $\bm{f} : \realnumbers^m \to
    \realnumbers^n$, then the Jacobian matrix $\bm{J} \in \realnumbers^{n
      \times m}$ of $\bm{f}$ is defined such that:
    \begin{equation}
      J_{i, j} = \frac{\partial}{\partial x_j} f(\bm{x})_i
    \end{equation}
\end{itemize}

\subsubsection{Hessian matrix}
\begin{itemize}
  \item the \textbf{second derivative} is the derivative of the derivative
  \item in the single dimension, we can denote it by $f''(x)$
  \item otherwise, $\frac{\partial^2}{\partial x_i \partial x_j}$
  \item the derivative with respect to $x_i$ of the derivative of $f$ with
    respect to $x_j$
  \item \textbf{second derivative test}
  \item the latest derivative is leftmost on the denominator
  \item we can think of it as the measure of \textbf{curvature}
  \item the matrix containing all the second derivatives of a function $f$ is
    called the \textbf{Hessian matrix} $\bm{H}(f)(\bm{x})$:   
    \begin{equation}
      \bm{H}(f)(\bm{x})_{i, j} = \frac{\partial^2}{\partial x_i \partial x_j}
      f(\bm{x})
    \end{equation}
  \item the Hessian is the Jacobian of the gradient
  \item the Hessian matrix is symmetric at $H_{i, j} = H_{j_i}$ since partial
    differentiation is commutative
  \item most functions we encounter in deep learning have a symmetric Hessian
    almost everywhere
  \item further readings can be had in pg. 85-89
\end{itemize}

\subsubsection{Newton's method}
\begin{itemize}
  \item full name: Newton-Raphson method
  \item a method for finding succssively better approximations to the roots
    (solutions) of a real-valued function
  \item i.e., $x^* : f(x) = 0$
  \item Newton's method is based on using a second-order Taylor series
    expansion to approximate $f(\bm{x})$ near some point $\bm{x}^{(0)}$:
    \begin{equation}
      \bm{x}^* = \bm{x}^{(0)} -
      \bm{H}(f)(\bm{x}^{(0)})^{-1}\nabla_{\bm{x}}f(\bm{x}^{(0)})
    \end{equation}
  \item \textbf{it might halt at a saddle point though}
  \item appropriate when the nearby critical point is minimum (all the
    eigenvalues of the Hessian are positive)
\end{itemize}

Optimization algorithms that use only the gradient are called
\textbf{first-order optimization algorithms}, while those that also use the
Hessian matrix, such as Netwon's method, are called \textbf{second-order
  optimization algorithms}.

In the context of deep learning, we may find some guarantees by restricting
ourselves to functions that are either \textbf{Lipschitz continuous} or have
Lipschitz continuous derivatives. A Lipschitz continuous function is a function
$f$ whose rate of change is bounded by a Lipschitz constant $\mathcal{L}$:
\begin{equation}
  \forall \bm{x}, \forall \bm{y},
  | f(\bm{x}) - f(\bm{y}) | \leq \mathcal{L} \| \bm{x} - \bm{y} \|_2
\end{equation}

The above is useful because it enables us to quantify our assumption that a
small change in the input made by an algorithm such as gradient descent will
have a small change in the output.

\section{Constrained Optimization}
\begin{itemize}
  \item finding maximal or minimal value of $f(\bm{x})$ for values of $\bm{x}$
    in some set S
  \item this is known as \textbf{constrained optimization}
  \item points $\bm{x}$ that lie within the set S are called \textbf{feasible}
    points in constrained optimization terminology
  \item a common approach in such situations is to impose a norm constraint,
    such as $\|\bm{x}\| \leq 1$
  \item specifics can be found at pg. 91 onwards
\end{itemize}

\section{Ex: Linear Least Squares}
Find the value of $\bm{x}$ that minimizes:
\begin{equation}
  f(\bm{x}) = \frac{1}{2} \| \bm{Ax} - \bm{b} \|^2
\end{equation}
Specialized linear algebra algorithms can solve this problem efficiently;
however, we can also explore how to solve it using gradient-based optimization
as a simple example of how these techniques work.

First, we need to obtain the gradient:
\begin{equation}
  \nabla_{\bm{x}} f(\bm{x}) = \bm{A}^\intercal (\bm{Ax} - \bm{b}) 
  = \bm{A}^\intercal\bm{Ax} - \bm{A}^\intercal\bm{b}
\end{equation}

We can then follow this gradient downhill, taking small steps at a time. See
the following algorithm for details:
\begin{algorithm}
  \KwData{the aforementioned function $f$, an arbitrary point $\bm{x}$, small
    positive numbers $\epsilon$, $\delta$ for step size and tolerance
    respectively}
  \KwResult{the (hopefully) minimized value}
  \While{$\| \bm{A}^\intercal\bm{Ax} - \bm{A}^\intercal\bm{b} \|_2 \geq \delta$} {
    $\bm{x} \leftarrow \bm{x} - \epsilon(\bm{A}^\intercal\bm{Ax} - \bm{A}^\intercal\bm{b})$
  }
  \caption{An algorithm to minimize the function $f$ with respect to $\bm{x}$
    using gradient descent}
\end{algorithm}

Now suppose we want to minimize the same function, but subject to the
constraint $\bm{x}^\intercal\bm{x} \leq 1$. To do so, we introduce the
Lagrangian:
\begin{equation}
  L(\bm{x},\lambda) = f(\bm{x}) + \lambda(\bm{x}^\intercal\bm{x} - 1).
\end{equation}
We can now solve the problem
\begin{equation}
  \min_x \max_{\lambda, \lambda \geq 0} L(\bm{x}, \lambda).
\end{equation}

The smallest-norm solution to the unconstrained least-squares problem may be
found using the Moore-Penrose pseudoinverse: $\bm{x} = \bm{A}^+\bm{b}$. If this
point is feasible, then it is the solution to the constrained problem.
Otherwise, we must find a solution where the constraint is active. By
differentiating the Lagrangian with respect to $\bm{x}$, we obtain:
\begin{equation}
  \bm{A}^\intercal\bm{Ax} - \bm{A}^\intercal\bm{b} + 2\lambda\bm{x} = 0.
\end{equation}

This tells us that the solution will take the form
\begin{equation}
  \bm{x} = (\bm{A}^\intercal\bm{A} + 2\lambda\bm{I})^{-1}\bm{A}^\intercal\bm{b}.
\end{equation}

The magnitude of $\lambda$ may be chosen such that the result obeys the
constraint. We can find this value by performing gradient ascent on $\lambda$.
To do so, observe:
\begin{equation}
  \frac{\partial}{\partial\lambda} L(\bm{x},\lambda) = \bm{x}^\intercal\bm{x} -
  1.
\end{equation}

When the norm exceeds 1, this derivative is positive, so to follow the
derivative uphill and increase the Lagrangian with respect to $\lambda$, we
increase $\lambda$. Because the coefficient on the $\bm{x}^\intercal\bm{x}$
penalty has increased, solving the linear equation for $\bm{x}$ will now yield
a solution with a smaller norm. The process of solving the linear equation and
adjusting $\lambda$ continues until $\bm{x}$ has the correct norm and the
derivative on $\lambda$ is 0.
\end{document}
