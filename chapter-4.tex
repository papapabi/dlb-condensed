\documentclass[11pt, twocolumn]{report}
\usepackage[margin=0.75in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\def\realnumbers{\mathbb{R}}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\setcounter{chapter}{3}

\chapter{Numerical Computation}
Machine learning algorithms typically require a high amount of numerical
computation. This typically refers to algorithms that update estimates of the
solution via an iterative process, rather than analytically deriving a formula
toprovide a symbolic expression for the correct (and exact) solution. Common
operations include \textbf{optimization} (finding a value of an argument that
minimizes or maximizes a function) and \textbf{solving systems of linear
  equations}. The mere evaluation of a mathematical function on a computer can
be difficult when the function's domain involves real numbers, which cannot be
represented precisely using a finite amount of memory.

\section{Overflow and Underflow}
The fundamental difficulty in performing continuous math operations on a
digital computer is that we need to represent infinitely many digits of a real
number with a finite number of bit patterns, depending on what is available on
a particular machine. This means that for almost all real numbers, we incur
some approximation error just by representing the number in the computer. In
most cases this is rounding error.  Rounding error can cause algorithms that
work in theory to fail in practice if they are not designed to minimize the
accumulation of rounding error.

\textbf{Underflow}
\begin{itemize}
  \item underflow occurs when numbers near zero are rounded to zero
  \item this is a problem because we want to avoid dividing by zero, among
    other things
\end{itemize}

\textbf{Overflow}
\begin{itemize}
  \item overflow occurs when numbers with a large magnitude are approximated as
    $\infty$ or $-\infty$
  \item further arithmetic will usually change these values to NaN values
\end{itemize}

Developers can simply rely on low-level libraries that provide stable
implementations.  Theano is an example of a software package that automatically
detects and stabilizes many common numerically unstable expressions that arise
in the context of deep learning.

\section{Poor Conditioning}
\begin{itemize}
  \item conditining refers to how rapidly a function changes with respect to
    small changes in its inputs
  \item \textbf{rounding errors in the inputs can cause large changes in the
      output}
  \item Consider the function $f(\bm{x}) = \bm{A}^{-1}\bm{x}$. When $\bm{A} \in
    \realnumbers^{n \times n}$ has an eigenvalue decomposition, its
    \textbf{condition number} is:
    \begin{equation}
      \max_{i, j} \left|\frac{\lambda_i}{\lambda_j}\right|
    \end{equation}
    which is the ratio of the largest and smallest eigenvalue
  \item when the condition number is large, matrix inversion is particularly
    sensitive to error in the input
  \item note that this sensitivity is an intrinsic property of the matrix
    itself, not the result of rounding error during matrix inversion
\end{itemize}

\section{Gradient-Based Optimization}
\begin{itemize}
  \item most deep learning algorithms involve optimization of some sort
  \item optimization refers to the task of either minimizing or maximizing
    some function $f(\bm{x})$ by altering $\bm{x}$
  \item most applications focus on minimizing $f$ though
  \item the function we want to minimize is called the \textbf{objective
      function/criterion/cost function/loss function/error function}
  \item note that some ML publications assign special meaning to each of said
    terms
  \item we typically denote the value that minimizes/maximizes a function with
    a superscript $*$, for instance, $\bm{x}^* = \argmin f(\bm{x})$
  \item the \textbf{derivative} is, in simplest terms, the slope of $f$
  \item the \textbf{derivative} specifies how to scale a small change in the
    input to obtain the corresponding change in the output: $f(x + \epsilon)
    \approx f(x) + \epsilon f'(x)$
  \item when $f'(x) = 0$, we are at a \textbf{critial point}
  \item local minimum: a point where $f(x)$ is lower compared to all
    neighboring points, so it is no longer possible to decrease $f(x)$ by taking
    infinitesimal steps
  \item absolute minimum: the absolute lowest value of $f(x)$
  \item local maximum, absolute maximum
  \item critical points that are neither maxima nor minima are called
    \textbf{saddle points}, which have neighbors that are both higher and lower
    than the point itself
\end{itemize}

\subsection{Gradient Descent}
\begin{itemize}
  \item a first-order iterative optimization algorithm for finding the minimum
    of a function
  \item "\textit{reduce $f(x)$ by moving in small steps with the opposite sign
      of the derivative}"
  \item consicely, $f(x - \epsilon \text{sign}(f'(x)))$
\end{itemize}

\textbf{Why the heck must we know this?}
\begin{itemize}
  \item in the context of deep learning, we optimize functions that may have
    many local minima that aren't optimal and many saddle points surrounded by
    very flat regions
  \item this makes optimization difficult
  \item not to mention when the input is multidimensional
  \item we therefore usually settl for a value of $f$ that is very low but may
    or may not be necessarily a minima in the formal sense
  \item we often minimize functions that have multiple inputs $f :
    \realnumbers^n \to \realnumbers$ (for the minimization to make sense the
    output must be scalar -- singular)
\end{itemize}

\subsection{Gradient Descent - Multiple Inputs}
\begin{itemize}
  \item for functions with multiple inputs we use \textbf{partial derivatives}
  \item the partial derivative $\frac{\partial}{\partial x_i} f(\bm{x})$
    measure how $f$ changes as only the variable $x_i$ inreases at point
    $\bm{x}$
  \item the \textbf{gradient} of $f$ is then the vector containing all the
    partial derivatives, denoted $\nabla_{\bm{x}} f(\bm{x})$
  \item element $i$ of the gradient is the partal derivative of $f$ with
    respect to $x_i$
  \item *in multiple dimensions, critical points are points where every element
    of the gradient is equal to zero
  \item there are many methods in this area, most notably, \textbf{steepest
      descent}, \textbf{line search}
  \item gradient descent is limited to optimization in continuous
    spaces, but the generally concept of repeatedly making a small move toward
    better configs can be generalized to discrete spaces
\end{itemize}

\subsection{Beyond the Gradient: Jacobian and Hessian Matrices}
\subsubsection{Jacobian matrix}
\begin{itemize}
  \item the matrix containing all partial derivatives of a function $f$ is
    called the \textbf{Jacobian matrix}
  \item specifically: if we have a function $\bm{f} : \realnumbers^m \to
    \realnumbers^n$, then the Jacobian matrix $\bm{J} \in \realnumbers^{n
      \times m}$ of $\bm{f}$ is defined such that:
    \begin{equation}
      J_{i, j} = \frac{\partial}{\partial x_j} f(\bm{x})_i
    \end{equation}
\end{itemize}

\subsubsection{Hessian matrix}
\begin{itemize}
  \item the \textbf{second derivative} is the derivative of the derivative
  \item in the single dimension, we can denote it by $f''(x)$
  \item otherwise, $\frac{\partial^2}{\partial x_i \partial x_j}$
  \item the derivative with respect to $x_i$ of the derivative of $f$ with
    respect to $x_j$
  \item the latest derivative is leftmost on the denominator
  \item we can think of it as the measure of \textbf{curvature}
  \item the matrix containing all the second derivatives of a function $f$ is
    called the \textbf{Hessian matrix} $\bm{H}(f)(\bm{x})$:   
    \begin{equation}
      \bm{H}(f)(\bm{x})_{i, j} = \frac{\partial^2}{\partial x_i \partial x_j}
      f(\bm{x})
    \end{equation}
  \item the Hessian is the Jacobian of the gradient
  \item the Hessian matrix is symmetric at $H_{i, j} = H_{j_i}$ since partial
    differentiation is commutative
  \item most functions we encounter in deep learning have a symmetric Hessian
    almost everywhere
\end{itemize}

\end{document}
