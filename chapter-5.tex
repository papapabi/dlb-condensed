\documentclass[11pt, twocolumn]{report}
\usepackage[margin=0.75in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\def\realnumbers{\mathbb{R}}
\def\expectation{\mathbb{E}}
\def\Var{\mathrm{Var}}
\def\statistic{\hat{\bm{\theta}}}
\def\parameter{\bm{\theta}}

\begin{document}
\setcounter{chapter}{4}
\chapter{Machine Learning Basics}

Overview:
\begin{itemize}
  \item what is a learning algorithm?
  \item fitting training data (to a model) is a different beast altogether from
    finding new patterns that generalize to new data
  \item for more reading, read Murphy (2012) or Bishop (2006)
  \item most ML algorithms have settings called \textit{hyperparameters}
  \item \textbf{machine learning is essentially a form of applied statistics
      with an increased emphasis on the use of computers to statistically
      estimate complicated functions and a decreased emphasis on proving
      confidence intervals around these functions}
  \item frequentist estimators and Bayseian inference
  \item supervised vs unsupervised learning
  \item most ML algorithms are based on an optimization algorithm called
    \textbf{stochastic gradient descent}
  \item the main components for an ML algorithm are: an optimization algorithm,
    a cost function, a model, a dataset
  \item factors that limit the ability of traditional machine learning to
    generalize
\end{itemize}

\section{Learning Algorithms}
A machine learning algorithm is an algorithm that is able to learn from data.
But what do we mean by \textit{learning}? Mitchell (1997) provides a succinct
definition:

"A computer program is said to learn from experience $E$ with
respect to some class of tasks $T$ and performance measure $P$, if its
performance at tasks in $T$ (as measured by $P$) improves with experience $E$."

\subsection{The Task, $T$}
\begin{itemize}
  \item the process of learning itself is \textbf{not} the task
  \item learning is the means of attaining the ability to perform the task
  \item if we want a robot to walk, then walking is the task
  \item machine learning tasks are usually describe in terms of how the machine
    learning system should process an \textbf{example}
  \item an example is a collection of \textbf{features} that have been
    quantitatively measured from some object or event that we want the machine
    learning to process
  \item we typically represent an example as a vector $\bm{x} \in
    \realnumbers^n$ where each entry $x_i$ of the vector is another feature
  \item the features of an image are usually the value of the pixels in the
    image
  \item there are many kinds of tasks that can be solved with machine
    learning
\end{itemize}

\subsubsection{Examples of tasks}

\underline{Classification}
\begin{itemize}
  \item specify which of $k$ categories some input belongs to
  \item we ask the system to produce some function $f : \realnumbers^n \to
    \{1,...,k\}$
  \item when $y = f(\bm{x})$, assigns an input described by vector $\bm{x}$ to
    a category identified by numeric code $y$ to a category identified by
    numeric code $y$
  \item an example is an input image, and the output is a numeric code
    identifying the object in the image
  \item object recognition is the same basic tech as facial recognition
\end{itemize}

\underline{Regression}
\begin{itemize}
  \item predict a numerical value given some input
  \item $f : \realnumbers^n \to \realnumbers$
  \item similar to classification save for the output (categorical vs numeric)
  \item an example is the prediction of future prices of produce
  \item these kinds of predictions are used for algorithmic trading
\end{itemize}

\underline{Transcription}
\begin{itemize}
  \item observe a relatively unstructured representaion of some kind of data
    and transcribe the information into discrete textual form
  \item optical character recognition
  \item image of text (.jpeg, .gif) to strings (e.g., in ASCII or Unicode)
  \item Google Street View uses deep learning to process address numbers in
    this way
  \item speech recognition, audio waveform to a sequence of characters 
\end{itemize}

\underline{Machine Translation}
\begin{itemize}
  \item input: sequence of symbols in some language
  \item output: sequence of symbols in another language
  \item commonly applied to natural languages, such as translating from English
    to French
\end{itemize}

\underline{Structured Output}
\begin{itemize}
  \item refers to the tasks where the output is a vector (or any data structure
    containing multiple values) with important relationships between the
    different elements
  \item one example is parsing -- mapping a natural language sentence into a
    tree that describes its grammatical structure by tagging its nodes as beig
    verbs, nouns, adverbs, and so on
  \item image captioning
\end{itemize}

\underline{Anomaly Detection}
\begin{itemize}
  \item sifts through a set of events or objects and flags some of them as
    being unusual or atypical
  \item an example is credit card fraud detection
  \item by modeling one's purchasing habits, a credit card company can detect
    misuse of your cards
  \item the thief's purchases will often come from a different probability
    distribution over purchase types than your own
\end{itemize}

\underline{Synthesis and Sampling}
\begin{itemize}
  \item generate new examples that are similar to those in the training data
  \item useful for media applications where generating large volumes of content
    by hand would be expensive, boring, or require too much time
  \item examples include texture generation for video games
  \item speech synthesis - written sentence $\to$ audio waveform containing the
    spoken version of said sentence
  \item we explicitly desire a large amount of variation in the output in order
    for it to seem more natural
\end{itemize}

\underline{Imputation of missing values}
\begin{itemize}
  \item the algorithm is given a new example $\bm{x} \in \realnumbers^n$, but
    with some entries $x_i$ of $\bm{x}$ missing
  \item the algorithm then must provide a prediction of the values of the
    missing entries\\\\\\
\end{itemize} 

\underline{Denoising}
\begin{itemize}
  \item the algorithm is given a \textit{corrupted example} $\widetilde{\bm{x}}
    \in \realnumbers^n$ obtained by an unknown corruption process from a
    \textit{clean example} $\bm{x} \in \realnumbers^n$
  \item the learner must predict the clean example from its corrupted version 
  \item more generally, predict the probability distribution $p(\bm{x} |
    \widetilde{\bm{x}})$
\end{itemize}

\underline{Density estimation}
\begin{itemize}
  \item the algorithm is asked to learn a function $p_{model} : \realnumbers^n
    \to \realnumbers$, where $p_{model}(\bm{x})$ can be interpreted as a pdf or
    pmf depending on the nature of $\bm{x}$ (continuous vs discrete)
  \item basically allows us to capture what distribution the examples were
    drawn from
  \item the algorithm needs to learn the structure of the training data
  \item after getting the pdf/pmf, a lot of othe tasks can be solved as well
    (like value imputation)
  \item in practice though, density estimation does not always enable us to
    solve all related tasks, because in many cases operations on $p(\bm{x})$
    are computationally intractable
\end{itemize}

There are many other types of tasks that are not mentioned, this is just to
give you a general idea of what is possible with machine learning.

\subsection{The Performance Measure, $P$}
\begin{itemize}
  \item usually, $P$ is speciic to task $T$
  \item \textbf{accuracy} - the proportion of examples for which the model
    produces the 'correct' output
  \item \textbf{error rate} - the proportin of examples for which the model
    produces the 'wrong' output
  \item the error rate is also known as the expected \textbf{0-1 loss} (0 if
    correctly classified and 1 if it is not)
  \item there are other tasks like density estimation where it does not make
    sense to measure accuracy, instead we use a metric that gives the model a
    real-valued score
  \item we get these performance metrics using the \textbf{test set} of data
  \item this test set is separate from the \textbf{training set}
  \item in some cases it is difficult to quantify performance metrics
  \item in a transcription task, should accuracy be aimed at trancscribing
    entire sequences or should a more fine-grained approach be used, as in
    partial credits for getting some of the elements correctly?
  \item in a regression task, should we penalize a system more for making
    frequent medium-sized mistakes or if it rarely makes very large mistakes?
  \item these kind of design choices largely depend on the application of the
    system
  \item in other cases measuring it is impractical even
\end{itemize}

\subsection{The Experience, $E$}
Machine learning algorithms can be broadly categorized as \textbf{unsupervised}
or \textbf{supervised} by what kind of experience they are allowed to have
during the learning process.

Most of what learning algorithms in this book can be understood as being
allowed to experience an entire \textbf{dataset}. A dataset is a collection of
many examples. Sometimes examples are known as data points.

Example: The Iris Dataset
\begin{itemize}
  \item a collection of measurements of different parts of 150 iris plants
  \item each individual plant = one example
  \item there are features within each example: things like sepal length, sepal
    width, petal length, and petal height
  \item the dataset also records which species each plant belongs to (there are
    three species)
\end{itemize}

\subsubsection{Unsupervised learning algorithms}
\begin{itemize}
  \item they experience a dataset containing many features
  \item then they themselves learn useful properties of the structure of this
    dataset a la inference
  \item we usually want to learn the entire probability distribution that
    generated the dataset
\end{itemize}

\subsubsection{Supervised learning algorithms}
\begin{itemize}
  \item same as unsupervised but each example is also associated with a
    \textbf{label}
  \item the Iris dataset is annotated with the species of each iris plant
  \item a supervised learning algorithm can study the Iris dataset and learn to
    classify iris plants into three different species based on their
    measurements and whatnot
\end{itemize}

\subsubsection{Supervised vs Unsupervised}
\begin{itemize}
  \item roughly speaking, unsupervised learning involves observing several
    examples of a random vector $\bm{x}$ and attempting to learn the probability
    distribution $p(\bm{x})$
  \item supervised learning involves observing several examples of a random
    vector $\bm{x}$ and an associated value or vector $\bm{y}$, then learning
    to predict $\bm{y}$ from $\bm{x}$, usually by estimating $p(\bm{y} |
    \bm{x})$
  \item the term 'supervised' comes from an instructor (you) 'showing' (giving
    examples $\bm{y}$) the student (the machine learning system) what to do
  \item in unsupervised learning you don't show the system what to do, it'll
    automagically try to infer stuff based from the dataset
  \item supervised learning is tedious because you have to individually label
    each example, which might be costly
  \item they aren't formally defined terms though
  \item traditionaly, people refer to \textbf{regression, classification, and
      structured output problems} as supervised learning
  \item \textbf{density estimation} in support of other tasks is unsupervised
    learning
\end{itemize}

\subsubsection{Other variants of learning}
\begin{itemize}
  \item there are other variants such as
  \item semi-supervised learning - some are labeled, some are not
  \item multi-instance learning - the entire collection is labeled or not
    containg an example of a clss, but the individual members aren't labeled
  \item reinforcement leaning - there is a feedback loop between the learning
    system and its experiences; you reward the system for producing the correct
    output and punish it for the wrong output
\end{itemize}

\subsubsection{The Dataset}
\begin{itemize}
  \item datasets are a collection of examples -> examples are a collection of
    features
  \item one common way to describe a dataset is with a \textbf{design matrix}
  \item a design matrix is a matrix containing a different example each row
  \item for instance, the Iris dataset contains 150 examples with 4 features
    for each example
  \item so we can represent these information as a design matrix $\bm{X} \in
    \realnumbers^{150 \times 4}$, where $X_{i, 1}$ is the sepal length, and so
    on
  \item most of the learning algorithms in this book operate on design matrices
  \item it is not always possible for the sub-vectors (each row) of the matrix
    to be of the same size
  \item in that case rather than describing the dataset as a matrix with $m$
    rows, we describe it a s a set containing $m$ elements:
    ${\bm{x}^{(1)},...\bm{x}^{(m)}}$
  \item this notation does not imply that any two example vectors within the
    set have the same size
  \item for labels in supervised learning, it can be anything: numeric codes,
    for categories, or strings, etc
  \item there is no rigid taxonomy for datasets and experiences, so one can be
    creative with how they are represented so long as it works well
\end{itemize}

\subsection{Example: Linear Regression}
\begin{itemize}
  \item an ML algorithm is an algorithm that is capable of improving a
    computer's performance at some task via experience
  \item to make this more concrete, let us examine \textbf{linear regression}
  \item as it name implies, it is a regression task
  \item to goal is to build a system thas has a function $f : \bm{x} \in
    \realnumbers^n \to y \in \realnumbers$
  \item the output of the linear regression is a linear function of the input
  \item we define the output to be:
    \begin{equation}
      \hat{y} = \bm{w}^\intercal\bm{x},
    \end{equation}
    where $\hat{y}$ is the predicted value, and $\bm{w} \in \realnumbers^n$ is
    a vector of \textbf{parameters}
  \item parameters are values that control the behavior of the system
  \item $\bm{w}$ is the coefficient matrix, and we can think of it as a set of
    \textbf{weights} that determine how much each feature affects the final
    prediction
  \item if a feature's weight has a large magnitude, that means it has a large
    effect on the prediction
  \item thus we have a definition for task $T$: predict $y$ from $\bm{x}$ by
    outputting $\hat{y} = \bm{w}^\intercal\bm{x}$
  \item one way of measuring linear regression is to computer the \textbf{mean
      squared error} of the model on the test set:
    \begin{equation}
      \text{MSE}_{\text{test}} = \frac{1}{m} \sum_i ( \hat{\bm{y}}^{(test)} -
      \bm{y}^{(test)})_i^2
    \end{equation}
  \item this error measure decreases to zero when $\hat{\bm{y}}^{(test)} =
    \bm{y}^{(test)}$
  \item so to minimize MSE, we can simply solve for where its gradient is
    $\bm{0}$:
    \begin{equation}
      \label{parameter_solution}
      \Rightarrow \bm{w} = \left(\bm{X}^{(train)\intercal}
        \bm{X}^{(train)}\right)^{-1} \bm{X}^{(train)} \bm{y}^{(train)}
    \end{equation}
  \item evaluation equation \ref{parameter_solution} constitutes a simple
    learning algorithm
  \item it is worth noting that linear regression has another parameter $b$
    (the constant) such that the mapping from features to predictions is now an
    \textbf{affine function}:
    \begin{equation}
      \hat{y} = \bm{w}^\intercal\bm{x} + b
    \end{equation}
  \item *an affine function is one that translates the whole graph by
    coordinate $a$, a linear transformation followed by translation
  \item the intercept term $b$ is often called the \textbf{bias} parameter
  \item this terminology comes from when the output tends to go to $b$ in the
    absence of any input, different from a \textbf{statistical bias}
\end{itemize}

\section{Capacity, Overfitting, and Underfitting}
The central challenge in any learning algorithm is that it must perform well on
\textit{new, previously unseen} inputs -- not only those in the training set.
The ability to perform well on previously unobserved input is known as
\textbf{generalization}.
Some terms:
\begin{itemize}
  \item training error - an error measure on the training set
  \item generalization/test error - expected value of the error on a new input
  \item generalization error is measured on a \textbf{test set}
  \item the question is: how can we affect performance on the test set when we
    can observe only the training set?
\end{itemize}

\subsubsection{Statistical learning theory}
\begin{itemize}
  \item we typically make a set of assumptions known collectively as
    \textbf{i.i.d}
  \item i.i.d = examples are independent (the probabilty that one event occurs
    in no way affects the probability of another event occuring) and
    identically distributed (drawn from the same probability distribution)
  \item assume that the distribution for the training set and test set is the
    same based from a single example
  \item we call this shared underlying distribution the \textbf{data-generating
      distribution}, denoted $p_{data}$
  \item collectively, these allow us to mathematically study the relationship
    between training set and test set (and their errors)
  \item so we can definitely assume that expected training error = expected
    test error
  \item procedure: sample the training set $\to$ use it to choose parameters to
    reduce training set error $\to$ sample the test set
  \item under this process, $\mathbb{E}[error_{test}] \geq
    \mathbb{E}[error_{training}]$
  \item for a machine learning algorithm to perform well, it has to: a)
    \textit{make the training error as small as possible}, and b) \textit{make
      the gap between training error and test error as small as possible}
  \item these two factors correspond to two central challenges in machine
    learning: \textbf{overfitting}, and \textbf{underfitting}
  \item underfitting: the model is not able to obtain a sufficiently low error
    on the training set
  \item overfitting: the gap between training and test error is too large
  \item capacity: the ability to fit a wide variety of mathematical functions
  \item models with low capacity may struggle to fit the training set
  \item models with high capacity can overfit by memorizing properties of the
    training set that do not serve them well on the test set
  \item conrolling capacity: choose a proper \textbf{hypothesis space}
  \item hypothesis space: the set of functions that the learning algorithm is
    allowed to select as its solution
  \item the linear regression algorithm has the set of all linear functions in
    its hypothesis space
  \item including other polynomials in its hypothesis space increases the
    model's capacity    
  \item this is fine because the output is still a linear function of its
    \textit{parameters}, even though the input may have quadratics or cubics et
    al
  \item ML algorithms generally perform the best when their capacity is
    appropriate for the true complexity of the task and the amount of training
    data they are provided with
  \item so a way to change capacity is by changing the number of input features
    it has, and simultaneously adding new parameters associated with those
    features
  \item \textbf{representational capacity} - the family of functions the
    learning algorithm can choose from when varying the parameters 
  \item \textbf{Occam's razor} - among competing hypotheses that explain known
    observations equally well, we should choose the "simplest" one
  \item statistical learning theory provides various means of quantifying model
    capacity
  \item the most well known is the \textbf{Vapnik-Chervonenkis dimension} or VC
    dimension
  \item VC dimension measures the capacity of a binary classifier
  \item VC dimension is defined as being the largest possible value of $m$ for
    which there exists a training set of $m$ different $\bm{x}$ points that the
    classifier can label arbitrarily
  \item the most important results in statistical learning theory shows that
    \textbf{the discrepancy between training error and generalization error is
      bounded from above by a quantity that grows as the model capacity grows
      but shrinks as the number of training examles increases}
  \item in my own terms, this means that the higher you make a model's
    capacity, after a certain point (the optimal capacity), it won't be able to
    generalize well (will have a high test error) (but of course, its training
    error will be super low)
  \item to balance it all out, we need to choose a sufficiently complex
    hypothesis to achieve low training/test error
  \item nonparametric learning algorithms are used to reach arbitrarily high
    capacity
  \item sometimes, they are just theoretical absractions that cannot be
    implemented in practice
  \item we can make a nonparametric learning algorithm by wrapping a parametric
    learning algorithm inside another algorithm that increases the number of
    parameters as needed
  \item \textbf{Bayes error rate} - the lowest possible error rate for any
    classifier of a random outcome (for instance, one of two categories) and is
    analogous to the irreducible error
  \item expected generalization \textbf{can never increase} as the number of
    training examples increases
  \item the more the merrier, even more so in machine learning, no exceptions
  \item any fixed parametric model with less then optimal capacity \textbf{will
      asymptote to an error value that exceed the Bayes error}
  \item the main takeaway in the above wall of text is that \textbf{capacity is
      very different from training set size, do not see them as the same thing}
\end{itemize}

\subsection{The No Free Lunch Theorem}
\begin{itemize}
  \item machine learning promises the find rules that are \textit{probably}
    correct about \textit{most} members of the set they are concerned about
  \item solutions aren't absolutes 
  \item because to logically infer a rule describing every member of a set, one
    must have information about every member of that set (which is not possible
    most times)
  \item the \textbf{no free lunch theorem} for machine learning states that,
    averaged over all possible data-generating distributions, every
    classification algorithm has the same error rate when classifying prevously
    unobserved points
  \item this nihlistic theorem states that, in some sense, no machine learning
    algorithm is universally any better than any other
  \item fortunately, these theorem only holds when we average over \textit{all}
    possible data-generating distributions
  \item if we make assumptions about the probability distributions we enconter
    in real-world applications, we can design learning algorithms that perform
    well on these distributions
  \item indeed, the goal of ML research is to understand what kinds of
    distributions are relevant to the "real world", and what kinds of ML
    algorithms perform well on data drawn from these distributions
\end{itemize}

\subsection{Regularization}
\begin{itemize}
  \item we can design learning algorithms that perform well on a specific task
    via building a set of preferences into the learning algorithm
  \item we can control the performance of our algorithms by choosing what kind
    of functions we allow them to operate on, in addition to controlling the
    amount of these functions
  \item we can also give a learning algorithm a preference for one solution
    over another in its hypothesis space
  \item recall that including/excluding members from the hypotheses space
    controls a model's capacity
  \item expressing preference for one function over another is a generalization
    of this
  \item excluding functions: expressing an infinitely strong preference against
    that function
  \item we can \textbf{regularize} a model that learns a function
    $f(\bm{x};\bm{\Theta})$ by adding a penalty called a \textbf{regularizer}
    to the cost function
  \item \textit{regularization is any modification made to a learning algorithm
      that is intended to reduce its generalization error but not its training
      error}
  \item look up: \textbf{weight decay} for linear regresson
  \item one of the central concerns of the field of machine learning, next to
    optimization
  \item there is no best form of regularization, as implied by the no free
    lunch theorem
  \item again we choose a form of regularization that is well-suited to the
    task we wish to solve
  \item but a wide range of tasks may all be solved efficiently using very
    general-purpose forms of regularization
\end{itemize}

\section{Hyperparameters and Validation Sets}
\textbf{Hyperparameters}
\begin{itemize}
  \item most ML algorithms have them
  \item they cannot be directly learned from the regular training process
  \item they are settings we can use to control the algorithm's behavior
  \item "higher-level properties" -- complexity, how fast it should learn
  \item usually fixed before the actual training begins
  \item though we can design a nested learning algorithm in which one learns
    the best hyperparameters for another leaning algorithm
  \item in the case of polynomial regression: the degree of the polynomial, and
    the $\lambda$ value used to control the strength of weight decay
  \item linear-poly regression: "we can always fit the training set better with
    a higher-degree polynomial and a weight decay setting of $\lambda = 0$ than
    we could with a lower-degree polynomial with a positive weight decay
    setting
\end{itemize}
\textbf{Validation Set}
\begin{itemize}
  \item minimizes overfitting
  \item "trains" the hyperparameters
  \item composed of examples coming from the same distribution as the training
    set
  \item we split the data set into two disjoint subsets
  \item one of these subsets is for the model to learn the parameters (training
    set)
  \item the other is to minimize overfitting (validation set)
  \item typically we use 80\% of the data for training and 20\% for validation
    (assuming you don't have test cases)
\end{itemize}

\section{Estimators, Bias and Variance}
There are many foundational concepts such as parameter estimation, bias, and
variance that are useful to formally characterize notions of generalization,
underfitting, and overfitting.

\subsection{Point Estimation}
\begin{itemize}
  \item the attempt to provide the single "best" prediction of some quantity of
    interest
  \item a single parameter, or a vector of parameters in some parametric model
  \item can also be a whole function
  \item denotation: point estimate $\statistic$ of parameter
    $\parameter$
  \item let $\{\bm{x}^{(1)},...,\bm{x}^{(m)}\}$ be a set of i.i.d. data points
    of size $m$
  \item a \textbf{point estimator} or \textbf{statistic} is any function of the
    data points:
    \begin{equation}
      \statistic_m = g(\bm{x}^{(1)},...,\bm{x}^{(m)})
    \end{equation}
  \item note that the definition does not require $g$ to return a value that is
    close to the true $\parameter$, or even that the range of $g$ be the same
    as the set of allowable values of $\parameter$
  \item any function thus qualifies to be an estimator
  \item the goal is to find a \textit{good} estimator whose output is close to
    the true underlying parameter(s) that generated the training data
  \item $\statistic$ is a random variable, since the data is drawn from
    a random process, and it is a function of the data
  \item as previously stated, sometimes, we are interested in performin
    function estimation (function approximation)
  \item function estimation: predict a varable $\bm{y}$ given an input vector
    $\bm{x}$
  \item estimates of the true underlying parameter is always uncertain because
    we only have a finite number of samples, in the sense that the statistics
    might differ at each resample, even if the same data-generating disribution
    is the same
\end{itemize}

\subsection{Bias}
\begin{itemize}
  \item bias measures the expected deviation from the true value of the funtion
    or parameter
  \item a feature of a statistic whereby the expected value of the results
    differs from the true underlying quantitative parameter being estimated
  \item defined as:
    \begin{equation}
      \text{bias}(\statistic_m) = \expectation{(\statistic_m)} - \parameter
    \end{equation}
  \item an estimator $\statistic_m$ is said to be \textbf{unbiased} if
    $\text{bias}(\statistic_m) = \bm{0}$, which implies
    $\expectation{(\statistic_m)} = \parameter$
  \item an estimator $\statistic_m$ is said to be \textbf{asymptotically
      unbiased} if $\lim_{m \to \infty} \text{bias}(\statistic_m) = \bm{0}$,
    which implies that $\lim_{m \to \infty} \expectation{(\statistic_m)} =
    \parameter$
  \item we want the bias to have a low value, as close to 0 as possible
  \item the derivation of unbiased estimators (or good enough estimators)
    usually comes from clever algebraic manipulation, making an expression
    inside (substitute the estimator in the definition of bias) look like the
    distribution in question, with different or same parameters
  \item A common estimator for the Bernoulli Distribution for $\theta$ is the
    mean of the samples (sum all sample values, divide by how many)
\end{itemize}

\subsubsection{Gaussian Distribution}
\begin{itemize}
  \item a common estimator for the Gaussian Distribution $\mu$ (the mean
    parameter) is known as the \textbf{sample mean}:
    \begin{equation}
      \hat{mu}_m = \frac{1}{m} \sum_{i=1}^m x_i
    \end{equation}
  \item the first estimator of $\sigma^2$ is known as the \textbf{sample
      variance}:
    \begin{equation}
      \hat{\sigma}_m^2 = \frac{1}{m} \sum_{i=1}^m \left(x_i -
        \hat{\mu}_m\right)^2
    \end{equation}
  \item it can then be derived that the \textbf{unbiased sample variance} is:
    \begin{equation}
      \tilde{\sigma}_m^2 = \frac{1}{m-1} \sum_{i=1}^m \left(x_i -
        \hat{\mu}_m\right)^2
    \end{equation}
  \item while unbiased estimators are clearly desirable, they are not always
    the "best" estimators
  \item we will often use biased estimators that possess some important
    properties
\end{itemize}

\subsection{Variance and Standard Error}
Variance and Standard Error
\begin{itemize}
  \item how much the estimator varies as a function of the data sample
  \item the \textbf{variance} of an estimator is simply:
    \begin{equation}
      \Var(\hat{\theta})
    \end{equation}
    where the random variable is the training set
  \item square root of the variance: \textbf{standard error}, item denoted
    SE$(\hat{\theta})$
  \item once more, these quantities provide a quantitative measure of how we
    would expect the estimate we compute from data to vary as we independently
    resample the dataset from the underlying data-generating process
  \item just as we like an estimator to exhibit low bias, we would also like it
    to have relatively low variance
  \item the expected degree of variation in any estimator is a source of error
    we want to quantify
  \item the standard error of the mean is given by:
    \begin{equation}
      \text{SE}(\hat{\mu}_m) = \sqrt{\Var\left[\frac{1}{m} \sum_{i=1}^m x_i
        \right]} = \frac{\sigma}{\sqrt{m}}
    \end{equation}
    where $\sigma^2$ is the true variance of the samples $x_i$
  \item the standard error is often estimated by using an estimate of $\sigma$
  \item unfortunately, estimates of $\sigma$ tend to underestimate the true
    standard deviation but are still used in practice
  \item for large $m$, the approximation is quite reasonable
  \item \textbf{the standard error of the mean is very useful in machine
      learning experiments}
  \item we often estimate the generalization error by computing the sample mean
    of the error on the test set
  \item the number of samples in the test set determines the accuracy of this
    estimate
  \item \textbf{the variance of the estimator decreases as a function of $m$,
      the number of example in the dataset}
  \item this is a common property of commonly-used estimators
\end{itemize}

\subsection{Balancing Bias and Variance to Minimize Mean Squared Error}
\begin{itemize}
  \item which estimator is better; one with more bias, or one with more
    variance?
  \item the most common way to negotiate this trade-off is to use
    cross-validation
  \item in the case of bias and variance, \textbf{mean squared error} (MSE)
    \begin{align}
      \text{MSE} &= \expectation[(\statistic_m - \parameter)^2]\\
      &= \text{Bias}(\statistic_m)^2 + \Var(\statistic_m)
    \end{align}
  \item measures the average of the squares of the difference between the
    estimator and what is estimated
  \item always non-negative, and values closer to zero are better
  \item the relationship between bias and variance is tightly linked to the
    machine learning concepts of capacity, underfitting, and overfitting
  \item \textbf{when generalization error is measured by the MSE, increasing
      capacity tends to decrease bias and increase variance}  
\end{itemize}

\subsection{Consistency}
\begin{itemize}
  \item ideally, we want our point estimates to converge to the true value of
    the corresponding parameters as the number of data points $m$ in our
    dataset increases
  \item an estimator is \textbf{consistent}, if it converges in probability to
    the true value of the parameter:
    \begin{equation}
      \text{p}\lim_{m \to \infty} \statistic_m = \parameter
    \end{equation}
  \item consistency ensures that the bias induced by the estimator diminishes
    as the number of data examples grows
  \item however, the reverse is not true -- asymptotic unbiasedness does not
    imply consistency
  \item the book has a really good example at page 129 illustrating this fact
\end{itemize}

\section{Maximum Likelihood Estimation}
The most common principle from which we can derive specific functions that are
good estimators for different models.

Consider a set of $m$ examples $\mathbb{X} = \{\bm{x}_1,...,\bm{x}_m\}$ drawn
independently from the true but unknown data-generating distirution
$p_{data}(\bm{x})$.

Let $p_{model}(\bm{x};\parameter)$ be a parametric family of probability
distributions over the same space indexed by $\parameter$. In other words,
$p_{model}(\bm{x};\parameter)$ maps any configuration $\bm{x}$ to a real number
estimating the true probability $p_{data}(\bm{x})$.

The maximum likelihood estimator for $\parameter$ is then defined as
\begin{align}
  \parameter_{ML} &= \underset{\parameter}{\arg\max}
  p_{model}(\mathbb{X};\parameter)\\
  &= \underset{\parameter}{\arg\max} \prod_{i=1}^m
  p_{model}(\bm{x}_i;\parameter).
\end{align}

This product over many probabilities can be inconvenient for various reasons.
For example, it is prone to numerical underflow. Taking the logarithm of the
likelihood does not change its argmax but does conveniently transform a product
into a sum:
\begin{equation}
  \parameter_{ML} = \underset{\parameter}{\arg\max} \sum_{i=1}^m \log p_{model}
  (\bm{x}_i; \parameter).
\end{equation}

Because the $\arg\max$ does not change when we rescale the cost function, we
can divide by $m$ to obtain a version of the criterion that is expressed as an
expectation with respect to the empirical distirbution $\hat{p}_{data}$ defined
by the training data:
\begin{equation}
  \parameter_{ML} = \underset{\parameter}{\arg\max} \expectation_{\bm{x}
    \sim \hat{p}_{data}} \log p_{model} (\bm{x};\parameter).
\end{equation}

Maximum likelihood is an attempt to make the model distribution match the
empirical distribution $\hat{p}_{data}$. Ideally, we would like to match the
true data-generating distribution $p_{data}$, but we have no direct access to
this distribution.

Minimizing the KL divergence is the same as maximizing the likelihood.

\subsection{Conditional Log-Likelihood and Mean Squared Error}
\begin{itemize}
  \item the maximum likelihood estimator can readily be generalized to estimate
    a conditional probability $P(\bm{y} | \bm{x}; \parameter)$ in order to
    predict $\bm{y}$ given $\bm{x}$
  \item the basis for most supervised learning
  \item the conditinal maximum likelihood estimator is:
    \begin{equation}
      \parameter_{ML} = \underset{\parameter}{\arg\max} P(\bm{Y} | \bm{X};
      \parameter)
    \end{equation}
    given $\bm{X}$ represents all our inputs and $\bm{Y}$ all our observed
    targets
  \item if the examples are assumed to be i.i.d., then this can be decomposed
    into:
    \begin{equation}
      \parameter_{ML} = \underset{\parameter}{\arg\max} \log P(\bm{y}^{(i)} |
      \bm{x}^{(i)}; \parameter)
    \end{equation}
  \item linear regression may be justified as a maximum likelihood procedure
\end{itemize}

\subsection{Properties of Maximum Likelihood}
\begin{itemize}
  \item shown to be the best estimator asymptotically (as the number of
    examples $m \to \infty$)
  \item MLE is consistent, under appropriate conditions:
  \item Firstly, the true distribution $p_{data}$ must lie within the model
    family $p_{model}(\cdot ; \parameter)$. Otherwise, no estimator can recover
    $p_{data}$.
  \item Secondly, the true distribution $p_{data}$ must correspond to exactly
    one value of $\parameter$. Otherwise, maximum likelihood will not be able
    to determine which value of $\parameter$ was used by the data-generating
    process (Even though the correct $p_{data}$ was recovered)
  \item consistent estimators differ in their \textbf{statistical efficiency}
  \item it can be shown (the Cramer-Rao lower bound) that no other consistent
    estimator has a lower MSE than the maximum likelihood estimator 
  \item for these reasons (consistency and efficiency), maximum likelihood is
    often considered the preferred estimator for use in machine learning
\end{itemize}


\section{Bayesian Statistics}
\begin{itemize}
  \item different from \textbf{frequentist statistics}
  \item considers all possible values of $\parameter$ when making a prediction
  \item unknown quantiies are treated probabilistically 
  \item "degrees of belief"
  \item 'the state of the world can always be updated'
  \item parameters are unknown and described probabilistcally, data are fixed
    (data is not random)
  \item the true parameter $\parameter$ is uncertain and therefore should be
    represented as a random variable
  \item before observing the data, we represent our knowledge of $\parameter$
    using the \textbf{prior probability distribution}, $p(\parameter)$
  \item now consider the we have a set of data samples $\{x_1,...,x_m\}$
  \item we can recover the effect of data on our belief about $\parameter$ by
    combining the data likelihood $p(x_1,...,x_m | \parameter)$ with the prior
    via Bayes' rule:
    \begin{equation}
      p(\parameter | x_1,...,x_m) = \frac{p(x_1,...,x_m |
        \parameter)p(\parameter)}{p(x_1,...,p_m)}
    \end{equation}
\end{itemize}

Bayseian vs Frequentist
\begin{itemize}
  \item frequentist: Data are a repeatable random sample - frequency exists.
    Underlying parameters are fixed (i.e., they remain constant during this
    repeatable sampling process.)
  \item bayseian: Data are fixed. Parameters are unknown and are thus described
    as random variables.
  \item Bayesian - $p$ the probability is not a value, it's a distribution
\end{itemize}



\section{End-of-Chapter Summary and Miscellany}
\begin{itemize}
  \item model - a mathematical formula with a number of parameters that need to
    be learned from the training data, that also need to apply to future,
    unseen data
  \item the crux of machine learning: \textbf{fitting a model to the data}
  \item the bigger the dataset, the better. this always applies
  \item there is no hard and fast rule for "bigger" but strive to get something
    in the ballpark of thousands, tens of thousands, to millions
  \item rule of thumb: 70\% should be training cases, 10\% test cases, and 20\%
    validation cases
  \item in the context of deep learning:
  \item training set: the data used to adjust the weights on a neural network
  \item validation set: minimizes overfitting
  \item testing set: data used only for testing the final solution in order to
    confirm the actual predictive power of the network
  \item validation sets
  \item bias: how much the expected value of the statistic differs from the
    true value, or the parameter
  \item variance: how much the expected estimate in question varies as
    we independently resample the dataset from the distribution
  \item the relationship between bias and variance is tightly linked to the
    machine learning concepts of capacity, underfitting, and overfitting
  \item as capacity increases, bias tends to decrease and variance tends to
    increase
  \item maximum likelihood is often considered as the preferred esitmator to
    use for machine learning
  \item Bayseian statistics vs Frequentist statistics
\end{itemize}
\end{document}
