\documentclass[11pt, twocolumn]{report}
\usepackage[margin=0.75in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\def\realnumbers{\mathbb{R}}

\begin{document}
\setcounter{chapter}{4}
\chapter{Machine Learning Basics}

Overview:
\begin{itemize}
  \item what is a learning algorithm?
  \item fitting training data (to a model) is a different beast altogether from
    finding new patterns that generalize to new data
  \item for more reading, read Murphy (2012) or Bishop (2006)
  \item most ML algorithms have settings called \textit{hyperparameters}
  \item \textbf{machine learning is essentially a form of applied statistics
      with an increased emphasis on the use of computers to statistically
      estimate complicated functions and a decreased emphasis on proving
      confidence intervals around these functions}
  \item frequentist estimators and Bayseian inference
  \item supervised vs unsupervised learning
  \item most ML algorithms are based on an optimization algorithm called
    \textbf{stochastic gradient descent}
  \item the main components for an ML algorithm are: an optimization algorithm,
    a cost function, a model, a dataset
  \item factors that limit the ability of traditional machine learning to
    generalize
\end{itemize}

\section{Learning Algorithms}
A machine learning algorithm is an algorithm that is able to learn from data.
But what do we mean by \textit{learning}? Mitchell (1997) provides a succinct
definition:

"A computer program is said to learn from experience $E$ with
respect to some class of tasks $T$ and performance measure $P$, if its
performance at tasks in $T$ (as measured by $P$) improves with experience $E$."

\subsection{The Task, $T$}
\begin{itemize}
  \item the process of learning itself is \textbf{not} the task
  \item learning is the means of attaining the ability to perform the task
  \item if we want a robot to walk, then walking is the task
  \item machine learning tasks are usually describe in terms of how the machine
    learning system should process an \textbf{example}
  \item an example is a collection of \textbf{features} that have been
    quantitatively measured from some object or event that we want the machine
    learning to process
  \item we typically represent an example as a vector $\bm{x} \in
    \realnumbers^n$ where each entry $x_i$ of the vector is another feature
  \item the features of an image are usually the value of the pixels in the
    image
  \item there are many kinds of tasks that can be solved with machine
    learning
\end{itemize}

\subsubsection{Examples of tasks}

\underline{Classification}
\begin{itemize}
  \item specify which of $k$ categories some input belongs to
  \item we ask the system to produce some function $f : \realnumbers^n \to
    \{1,...,k\}$
  \item when $y = f(\bm{x})$, assigns an input described by vector $\bm{x}$ to
    a category identified by numeric code $y$ to a category identified by
    numeric code $y$
  \item an example is an input image, and the output is a numeric code
    identifying the object in the image
  \item object recognition is the same basic tech as facial recognition
\end{itemize}

\underline{Regression}
\begin{itemize}
  \item predict a numerical value given some input
  \item $f : \realnumbers^n \to \realnumbers$
  \item similar to classification save for the output (categorical vs numeric)
  \item an example is the prediction of future prices of produce
  \item these kinds of predictions are used for algorithmic trading
\end{itemize}

\underline{Transcription}
\begin{itemize}
  \item observe a relatively unstructured representaion of some kind of data
    and transcribe the information into discrete textual form
  \item optical character recognition
  \item image of text (.jpeg, .gif) to strings (e.g., in ASCII or Unicode)
  \item Google Street View uses deep learning to process address numbers in
    this way
  \item speech recognition, audio waveform to a sequence of characters 
\end{itemize}

\underline{Machine Translation}
\begin{itemize}
  \item input: sequence of symbols in some language
  \item output: sequence of symbols in another language
  \item commonly applied to natural languages, such as translating from English
    to French
\end{itemize}

\underline{Structured Output}
\begin{itemize}
  \item refers to the tasks where the output is a vector (or any data structure
    containing multiple values) with important relationships between the
    different elements
  \item one example is parsing -- mapping a natural language sentence into a
    tree that describes its grammatical structure by tagging its nodes as beig
    verbs, nouns, adverbs, and so on
  \item image captioning
\end{itemize}

\underline{Anomaly Detection}
\begin{itemize}
  \item sifts through a set of events or objects and flags some of them as
    being unusual or atypical
  \item an example is credit card fraud detection
  \item by modeling one's purchasing habits, a credit card company can detect
    misuse of your cards
  \item the thief's purchases will often come from a different probability
    distribution over purchase types than your own
\end{itemize}

\underline{Synthesis and Sampling}
\begin{itemize}
  \item generate new examples that are similar to those in the training data
  \item useful for media applications where generating large volumes of content
    by hand would be expensive, boring, or require too much time
  \item examples include texture generation for video games
  \item speech synthesis - written sentence $\to$ audio waveform containing the
    spoken version of said sentence
  \item we explicitly desire a large amount of variation in the output in order
    for it to seem more natural
\end{itemize}

\underline{Imputation of missing values}
\begin{itemize}
  \item the algorithm is given a new example $\bm{x} \in \realnumbers^n$, but
    with some entries $x_i$ of $\bm{x}$ missing
  \item the algorithm then must provide a prediction of the values of the
    missing entries\\\\\\
\end{itemize} 

\underline{Denoising}
\begin{itemize}
  \item the algorithm is given a \textit{corrupted example} $\widetilde{\bm{x}}
    \in \realnumbers^n$ obtained by an unknown corruption process from a
    \textit{clean example} $\bm{x} \in \realnumbers^n$
  \item the learner must predict the clean example from its corrupted version 
  \item more generally, predict the probability distribution $p(\bm{x} |
    \widetilde{\bm{x}})$
\end{itemize}

\underline{Density estimation}
\begin{itemize}
  \item the algorithm is asked to learn a function $p_{model} : \realnumbers^n
    \to \realnumbers$, where $p_{model}(\bm{x})$ can be interpreted as a pdf or
    pmf depending on the nature of $\bm{x}$ (continuous vs discrete)
  \item basically allows us to capture what distribution the examples were
    drawn from
  \item the algorithm needs to learn the structure of the training data
  \item after getting the pdf/pmf, a lot of othe tasks can be solved as well
    (like value imputation)
  \item in practice though, density estimation does not always enable us to
    solve all related tasks, because in many cases operations on $p(\bm{x})$
    are computationally intractable
\end{itemize}

There are many other types of tasks that are not mentioned, this is just to
give you a general idea of what is possible with machine learning.

\subsection{The Performance Measure, $P$}
\begin{itemize}
  \item usually, $P$ is speciic to task $T$
  \item \textbf{accuracy} - the proportion of examples for which the model
    produces the 'correct' output
  \item \textbf{error rate} - the proportin of examples for which the model
    produces the 'wrong' output
  \item the error rate is also known as the expected \textbf{0-1 loss} (0 if
    correctly classified and 1 if it is not)
  \item there are other tasks like density estimation where it does not make
    sense to measure accuracy, instead we use a metric that gives the model a
    real-valued score
  \item we get these performance metrics using the \textbf{test set} of data
  \item this test set is separate from the \textbf{training set}
  \item in some cases it is difficult to quantify performance metrics
  \item in a transcription task, should accuracy be aimed at trancscribing
    entire sequences or should a more fine-grained approach be used, as in
    partial credits for getting some of the elements correctly?
  \item in a regression task, should we penalize a system more for making
    frequent medium-sized mistakes or if it rarely makes very large mistakes?
  \item these kind of design choices largely depend on the application of the
    system
  \item in other cases measuring it is impractical even
\end{itemize}
\end{document}

